{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦‘ Tru initialized with db url sqlite:///default.sqlite .\n",
      "ðŸ›‘ Secret keys may be written to the database. See the `database_redact_keys` option of `Tru` to prevent this.\n"
     ]
    }
   ],
   "source": [
    "#import statements\n",
    "from IPython.display import JSON\n",
    "\n",
    "# Imports main tools:\n",
    "from trulens_eval import TruChain, Feedback, Huggingface, Tru\n",
    "from trulens_eval.schema import FeedbackResult\n",
    "tru = Tru()\n",
    "tru.reset_database()\n",
    "\n",
    "# Imports from langchain to build app\n",
    "import bs4\n",
    "from langchain import hub\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.document_loaders import WebBaseLoader\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.schema import StrOutputParser\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "\n",
    "# Bocconi implementation\n",
    "import os\n",
    "import openai\n",
    "from langchain.document_loaders import UnstructuredMarkdownLoader\n",
    "from langchain.text_splitter import MarkdownHeaderTextSplitter\n",
    "from trulens_eval.feedback.provider import OpenAI\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from trulens_eval.app import App\n",
    "from trulens_eval.feedback import Groundedness\n",
    "\n",
    "# different retrievers\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.retrievers.self_query.base import SelfQueryRetriever\n",
    "from langchain.chains.query_constructor.base import AttributeInfo\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import LLMChainExtractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "#SET UP\n",
    "os.environ['OPENAI_API_KEY'] = \"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\"\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "#DOCUMENT LOADING\n",
    "file_path = \"../../Data/Scraping_Bocconi_converted_no_dup_check.md\"\n",
    "with open(file_path, 'r') as file:\n",
    "    markdown_content = file.read()\n",
    "\n",
    "#CREATE VECTOR STORE\n",
    "headers_to_split_on = [\n",
    "    (\"#\", \"Header 1\"),\n",
    "    (\"##\", \"Header 2\"),\n",
    "    (\"###\", \"Header 3\"),\n",
    "    (\"####\", \"Header 4\"),]\n",
    "markdown_splitter = MarkdownHeaderTextSplitter(\n",
    "    headers_to_split_on=headers_to_split_on)\n",
    "splits = markdown_splitter.split_text(markdown_content)\n",
    "vectorstore = Chroma.from_documents(documents=splits, embedding=OpenAIEmbeddings(\n",
    "))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "#CREATE RAG\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "#SUBSITUTE\n",
    "\n",
    "compressor = LLMChainExtractor.from_llm(llm)\n",
    "\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor,\n",
    "    base_retriever=vectorstore.as_retriever()\n",
    ")\n",
    "\n",
    "rag_chain_compressed = (\n",
    "    {\"context\": compression_retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/valedipalo/miniforge3/envs/aienv/lib/python3.10/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n",
      "/Users/valedipalo/miniforge3/envs/aienv/lib/python3.10/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n",
      "/Users/valedipalo/miniforge3/envs/aienv/lib/python3.10/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n",
      "/Users/valedipalo/miniforge3/envs/aienv/lib/python3.10/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": "\"Per effettuare il check-in in residenza, Ã¨ necessario compilare la procedura di check-in online su MyApplication entro la scadenza indicata e presentarsi alla reception con un documento di identitÃ  valido. Dopo aver effettuato l'ingresso in residenza, Ã¨ importante compilare la sezione Room check del check-in online per segnalare eventuali anomalie o malfunzionamenti.\""
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain_compressed.invoke(\"Come funziona per il checkin in residenza? \")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for Groundedness\ngroundedness_provider\n  Input should be a valid dictionary or instance of Provider [type=model_type, input_value=OpenAI(client=<openai.res...veF51', openai_proxy=''), input_type=OpenAI]\n    For further information visit https://errors.pydantic.dev/2.5/v/model_type",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValidationError\u001B[0m                           Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[7], line 12\u001B[0m\n\u001B[1;32m      8\u001B[0m \u001B[38;5;66;03m# select context to be used in feedback. the location of context is app specific.\u001B[39;00m\n\u001B[1;32m     10\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtrulens_eval\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mfeedback\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Groundedness\n\u001B[0;32m---> 12\u001B[0m grounded \u001B[38;5;241m=\u001B[39m \u001B[43mGroundedness\u001B[49m\u001B[43m(\u001B[49m\u001B[43mgroundedness_provider\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mOpenAI\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     13\u001B[0m \u001B[38;5;66;03m# Define a groundedness feedback function\u001B[39;00m\n\u001B[1;32m     14\u001B[0m f_groundedness \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m     15\u001B[0m     Feedback(grounded\u001B[38;5;241m.\u001B[39mgroundedness_measure_with_cot_reasons)\n\u001B[1;32m     16\u001B[0m     \u001B[38;5;241m.\u001B[39mon(context\u001B[38;5;241m.\u001B[39mcollect()) \u001B[38;5;66;03m# collect context chunks into a list\u001B[39;00m\n\u001B[1;32m     17\u001B[0m     \u001B[38;5;241m.\u001B[39mon_output()\n\u001B[1;32m     18\u001B[0m     \u001B[38;5;241m.\u001B[39maggregate(grounded\u001B[38;5;241m.\u001B[39mgrounded_statements_aggregator)\n\u001B[1;32m     19\u001B[0m )\n",
      "File \u001B[0;32m~/miniforge3/envs/aienv/lib/python3.10/site-packages/trulens_eval/feedback/groundedness.py:54\u001B[0m, in \u001B[0;36mGroundedness.__init__\u001B[0;34m(self, groundedness_provider)\u001B[0m\n\u001B[1;32m     52\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m groundedness_provider \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m     53\u001B[0m     groundedness_provider \u001B[38;5;241m=\u001B[39m OpenAI()\n\u001B[0;32m---> 54\u001B[0m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;21;43m__init__\u001B[39;49m\u001B[43m(\u001B[49m\n\u001B[1;32m     55\u001B[0m \u001B[43m    \u001B[49m\u001B[43mgroundedness_provider\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroundedness_provider\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     56\u001B[0m \u001B[43m    \u001B[49m\u001B[43mobj\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# for WithClassInfo\u001B[39;49;00m\n\u001B[1;32m     57\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniforge3/envs/aienv/lib/python3.10/site-packages/trulens_eval/utils/pyschema.py:610\u001B[0m, in \u001B[0;36mWithClassInfo.__init__\u001B[0;34m(self, class_info, obj, cls, *args, **kwargs)\u001B[0m\n\u001B[1;32m    606\u001B[0m     class_info \u001B[38;5;241m=\u001B[39m Class\u001B[38;5;241m.\u001B[39mof_class(\u001B[38;5;28mcls\u001B[39m, with_bases\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[1;32m    608\u001B[0m kwargs[CLASS_INFO] \u001B[38;5;241m=\u001B[39m class_info\n\u001B[0;32m--> 610\u001B[0m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;21;43m__init__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniforge3/envs/aienv/lib/python3.10/site-packages/pydantic/main.py:164\u001B[0m, in \u001B[0;36mBaseModel.__init__\u001B[0;34m(__pydantic_self__, **data)\u001B[0m\n\u001B[1;32m    162\u001B[0m \u001B[38;5;66;03m# `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\u001B[39;00m\n\u001B[1;32m    163\u001B[0m __tracebackhide__ \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[0;32m--> 164\u001B[0m \u001B[43m__pydantic_self__\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m__pydantic_validator__\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mvalidate_python\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mself_instance\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m__pydantic_self__\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mValidationError\u001B[0m: 1 validation error for Groundedness\ngroundedness_provider\n  Input should be a valid dictionary or instance of Provider [type=model_type, input_value=OpenAI(client=<openai.res...veF51', openai_proxy=''), input_type=OpenAI]\n    For further information visit https://errors.pydantic.dev/2.5/v/model_type"
     ]
    }
   ],
   "source": [
    "tru.reset_database()\n",
    "from trulens_eval.feedback.provider import OpenAI as fOpenAI\n",
    "import numpy as np\n",
    "\n",
    "# Initialize provider class\n",
    "openai = fOpenAI()\n",
    "\n",
    "# select context to be used in feedback. the location of context is app specific.\n",
    "\n",
    "from trulens_eval.feedback import Groundedness\n",
    "\n",
    "grounded = Groundedness(groundedness_provider=OpenAI())\n",
    "# Define a groundedness feedback function\n",
    "f_groundedness = (\n",
    "    Feedback(grounded.groundedness_measure_with_cot_reasons)\n",
    "    .on(context.collect()) # collect context chunks into a list\n",
    "    .on_output()\n",
    "    .aggregate(grounded.grounded_statements_aggregator)\n",
    ")\n",
    "\n",
    "# Question/answer relevance between overall question and answer.\n",
    "f_qa_relevance = Feedback(openai.relevance_with_cot_reasons, name = \"Answer relevance\").on_input_output()\n",
    "\n",
    "# Question/statement relevance between question and each context chunk.\n",
    "f_context_relevance = (\n",
    "    Feedback(openai.qs_relevance_with_cot_reasons, name = \"Context relevance\")\n",
    "    .on_input()\n",
    "    .on(context)\n",
    "    .aggregate(np.mean)\n",
    "    )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found more than one `BaseRetriever` in app:\n\t<class 'langchain.retrievers.contextual_compression.ContextualCompressionRetriever'> at first.steps.context.first\n\t<class 'langchain_core.vectorstores.VectorStoreRetriever'> at first.steps.context.first.base_retriever",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[10], line 6\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[38;5;66;03m# select context to be used in feedback. the location of context is app specific.\u001B[39;00m\n\u001B[1;32m      5\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtrulens_eval\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mapp\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m App\n\u001B[0;32m----> 6\u001B[0m context \u001B[38;5;241m=\u001B[39m \u001B[43mApp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mselect_context\u001B[49m\u001B[43m(\u001B[49m\u001B[43mrag_chain_compressed\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      8\u001B[0m tru\u001B[38;5;241m.\u001B[39mreset_database()\n\u001B[1;32m      9\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtrulens_eval\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mfeedback\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mprovider\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m OpenAI \u001B[38;5;28;01mas\u001B[39;00m fOpenAI\n",
      "File \u001B[0;32m~/miniforge3/envs/aienv/lib/python3.10/site-packages/trulens_eval/app.py:474\u001B[0m, in \u001B[0;36mApp.select_context\u001B[0;34m(cls, app)\u001B[0m\n\u001B[1;32m    472\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mtype\u001B[39m(app)\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__module__\u001B[39m\u001B[38;5;241m.\u001B[39mstartswith(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlangchain\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[1;32m    473\u001B[0m     \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtrulens_eval\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mtru_chain\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m TruChain\n\u001B[0;32m--> 474\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mTruChain\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mselect_context\u001B[49m\u001B[43m(\u001B[49m\u001B[43mapp\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    475\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mtype\u001B[39m(app)\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__module__\u001B[39m\u001B[38;5;241m.\u001B[39mstartswith(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mllama_index\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[1;32m    476\u001B[0m     \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtrulens_eval\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mtru_llama\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m TruLlama\n",
      "File \u001B[0;32m~/miniforge3/envs/aienv/lib/python3.10/site-packages/trulens_eval/tru_chain.py:249\u001B[0m, in \u001B[0;36mTruChain.select_context\u001B[0;34m(cls, app)\u001B[0m\n\u001B[1;32m    246\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCannot find any `BaseRetriever` in app.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    248\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(retrievers) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[0;32m--> 249\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m    250\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFound more than one `BaseRetriever` in app:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;130;01m\\t\u001B[39;00m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m+\u001B[39m \\\n\u001B[1;32m    251\u001B[0m         (\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;130;01m\\t\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mjoin(\u001B[38;5;28mmap\u001B[39m(\n\u001B[1;32m    252\u001B[0m             \u001B[38;5;28;01mlambda\u001B[39;00m lr: \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mtype\u001B[39m(lr[\u001B[38;5;241m1\u001B[39m])\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m at \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mlr[\u001B[38;5;241m0\u001B[39m]\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m, \n\u001B[1;32m    253\u001B[0m             retrievers)))\n\u001B[1;32m    254\u001B[0m     )\n\u001B[1;32m    256\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m (Select\u001B[38;5;241m.\u001B[39mRecordCalls \u001B[38;5;241m+\u001B[39m retrievers[\u001B[38;5;241m0\u001B[39m][\u001B[38;5;241m0\u001B[39m])\u001B[38;5;241m.\u001B[39mget_relevant_documents\u001B[38;5;241m.\u001B[39mrets\n",
      "\u001B[0;31mValueError\u001B[0m: Found more than one `BaseRetriever` in app:\n\t<class 'langchain.retrievers.contextual_compression.ContextualCompressionRetriever'> at first.steps.context.first\n\t<class 'langchain_core.vectorstores.VectorStoreRetriever'> at first.steps.context.first.base_retriever"
     ]
    }
   ],
   "source": [
    "#TESTING\n",
    "from trulens_eval.feedback.provider import OpenAI\n",
    "import numpy as np\n",
    "# select context to be used in feedback. the location of context is app specific.\n",
    "from trulens_eval.app import App\n",
    "context = App.select_context(rag_chain_compressed)\n",
    "\n",
    "tru.reset_database()\n",
    "from trulens_eval.feedback.provider import OpenAI as fOpenAI\n",
    "import numpy as np\n",
    "\n",
    "# Initialize provider class\n",
    "openai = fOpenAI()\n",
    "\n",
    "# select context to be used in feedback. the location of context is app specific.\n",
    "\n",
    "from trulens_eval.feedback import Groundedness\n",
    "\n",
    "# Question/statement relevance between question and each context chunk.\n",
    "f_context_relevance = Feedback(openai.qs_relevance).on_input().on(context).aggregate(numpy.min)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "tru_recorder = TruChain(rag_chain_base,\n",
    "                        app_id='Chain2_ChatApplication',\n",
    "                        feedbacks=[f_qa_relevance, f_context_relevance, f_groundedness])\n",
    "\n",
    "with tru_recorder as recording:\n",
    "    llm_response = rag_chain_base.invoke(\"Come funziona l'ingresso in residenza? \")\n",
    "\n",
    "display(llm_response)\n",
    "# The record of the ap invocation can be retrieved from the `recording`:\n",
    "\n",
    "rec = recording.get()  # use .get if only one record\n",
    "# recs = recording.records # use .records if multiple\n",
    "\n",
    "display(rec)\n",
    "\n",
    "records, feedback = tru.get_records_and_feedback(app_ids=[\"Chain2_ChatApplication\"])\n",
    "\n",
    "records.head()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model comparison\n",
    "### What is about ?\n",
    "*Constructing your language model application will likely involved choosing between many different options of prompts, models, and even chains to use. When doing so, you will want to compare these different options on different inputs in an easy, flexible, and intuitive way.*\n",
    "Langchian guide [Model Comparison](https://python.langchain.com/docs/guides/model_laboratory)\n",
    "### Implementation repo\n",
    "01/02/24 - Discovered\n",
    "\n",
    "### Advantages of doing this\n",
    "will be able to compare qualitatively the answers using different LLM\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## [Error] BaseRetriever\n",
    "`ValueError: Found more than one BaseRetriever in app:\n",
    "\t<class 'langchain.retrievers.contextual_compression.ContextualCompressionRetriever'> at first.steps.context.first\n",
    "\t<class 'langchain_core.vectorstores.VectorStoreRetriever'> at first.steps.context.first.base_retriever `\n",
    "\n",
    "### Implementation repo\n",
    "01/02/2024\n",
    "1-To try understanding the nature of this error I'll replicate an easier version of a RAG application and evaluate if the error is coming back\n",
    "2-Implement the same situation using the following colab [link](https://colab.research.google.com/github/truera/trulens/blob/main/trulens_eval/examples/quickstart/langchain_quickstart.ipynb#scrollTo=IPHUK9Xhnb4D)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "# required imports\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.prompts.chat import HumanMessagePromptTemplate, ChatPromptTemplate\n",
    "from trulens_eval import TruChain\n",
    "\n",
    "# typical langchain rag setup\n",
    "full_prompt = HumanMessagePromptTemplate(\n",
    "    prompt=PromptTemplate(\n",
    "        template=\n",
    "        \"Provide a helpful response with relevant background information for the following: {prompt}\",\n",
    "        input_variables=[\"prompt\"],\n",
    "    )\n",
    ")\n",
    "chat_prompt_template = ChatPromptTemplate.from_messages([full_prompt])\n",
    "\n",
    "llm = OpenAI(temperature=0.9, max_tokens=128)\n",
    "chain = LLMChain(llm=llm, prompt=chat_prompt_template, verbose=True)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "tru_recorder = TruChain(chain) #difference 1\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "aienv",
   "language": "python",
   "display_name": "aienv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}