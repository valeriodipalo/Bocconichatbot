{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6GRBzwNfnb38",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Langchain Quickstart\n",
    "\n",
    "In this quickstart you will create a simple LLM Chain and learn how to log it and get feedback on an LLM response.\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/truera/trulens/blob/main/trulens_eval/examples/quickstart/langchain_quickstart.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MJKBWDvCnb3_",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Setup\n",
    "### Add API keys\n",
    "For this quickstart you will need Open AI and Huggingface keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "! pip install trulens_eval==0.21.0 openai==1.3.7 langchain chromadb langchainhub bs4"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gkyOWMGVnb3_",
    "outputId": "9120ac19-c808-4c81-a92a-a390d990ffe9",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting trulens_eval==0.21.0\n",
      "  Downloading trulens_eval-0.21.0-py3-none-any.whl (645 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m646.0/646.0 KB\u001B[0m \u001B[31m5.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0ma \u001B[36m0:00:01\u001B[0m\n",
      "\u001B[?25hCollecting openai==1.3.7\n",
      "  Downloading openai-1.3.7-py3-none-any.whl (221 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m221.4/221.4 KB\u001B[0m \u001B[31m8.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting langchain\n",
      "  Downloading langchain-0.1.5-py3-none-any.whl (806 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m806.7/806.7 KB\u001B[0m \u001B[31m15.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m\n",
      "\u001B[?25hCollecting chromadb\n",
      "  Downloading chromadb-0.4.22-py3-none-any.whl (509 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m509.0/509.0 KB\u001B[0m \u001B[31m15.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: langchainhub in /Users/valedipalo/miniforge3/lib/python3.9/site-packages (0.1.14)\n",
      "Collecting bs4\n",
      "  Downloading bs4-0.0.2-py2.py3-none-any.whl (1.2 kB)\n",
      "Collecting sqlalchemy>=2.0.19\n",
      "  Downloading SQLAlchemy-2.0.25-cp39-cp39-macosx_11_0_arm64.whl (2.1 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m2.1/2.1 MB\u001B[0m \u001B[31m22.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\n",
      "\u001B[?25hCollecting merkle-json>=1.0.0\n",
      "  Downloading merkle_json-1.0.0-py3-none-any.whl (5.2 kB)\n",
      "Collecting python-dotenv>=1.0.0\n",
      "  Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
      "Collecting streamlit-extras>=0.2.7\n",
      "  Downloading streamlit_extras-0.3.6-py3-none-any.whl (66 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m66.0/66.0 KB\u001B[0m \u001B[31m2.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting alembic>=1.11.2\n",
      "  Downloading alembic-1.13.1-py3-none-any.whl (233 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m233.4/233.4 KB\u001B[0m \u001B[31m9.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting millify>=0.1.1\n",
      "  Downloading millify-0.1.1.tar.gz (1.2 kB)\n",
      "  Preparing metadata (setup.py) ... \u001B[?25ldone\n",
      "\u001B[?25hCollecting typing-inspect>=0.8.0\n",
      "  Using cached typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Collecting munch>=3.0.0\n",
      "  Downloading munch-4.0.0-py2.py3-none-any.whl (9.9 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /Users/valedipalo/miniforge3/lib/python3.9/site-packages (from trulens_eval==0.21.0) (4.8.0)\n",
      "Collecting requests>=2.31.0\n",
      "  Using cached requests-2.31.0-py3-none-any.whl (62 kB)\n",
      "Requirement already satisfied: streamlit>=1.27.0 in /Users/valedipalo/miniforge3/lib/python3.9/site-packages (from trulens_eval==0.21.0) (1.29.0)\n",
      "Requirement already satisfied: numpy>=1.23.5 in /Users/valedipalo/miniforge3/lib/python3.9/site-packages (from trulens_eval==0.21.0) (1.23.5)\n",
      "Collecting humanize>=4.6.0\n",
      "  Downloading humanize-4.9.0-py3-none-any.whl (126 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m126.8/126.8 KB\u001B[0m \u001B[31m5.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting streamlit-aggrid>=0.3.4.post3\n",
      "  Downloading streamlit_aggrid-0.3.4.post3-py3-none-any.whl (3.4 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m3.4/3.4 MB\u001B[0m \u001B[31m27.8 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\n",
      "\u001B[?25hCollecting pydantic<3,>=2\n",
      "  Downloading pydantic-2.6.0-py3-none-any.whl (394 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m394.2/394.2 KB\u001B[0m \u001B[31m12.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting langchain-core>=0.1.6\n",
      "  Downloading langchain_core-0.1.18-py3-none-any.whl (237 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m237.0/237.0 KB\u001B[0m \u001B[31m9.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting tqdm>=4.66.1\n",
      "  Using cached tqdm-4.66.1-py3-none-any.whl (78 kB)\n",
      "Collecting frozendict>=2.3.8\n",
      "  Downloading frozendict-2.4.0-cp39-cp39-macosx_11_0_arm64.whl (37 kB)\n",
      "Collecting dill>=0.3.7\n",
      "  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m116.3/116.3 KB\u001B[0m \u001B[31m4.8 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: anyio<4,>=3.5.0 in /Users/valedipalo/miniforge3/lib/python3.9/site-packages (from openai==1.3.7) (3.5.0)\n",
      "Requirement already satisfied: sniffio in /Users/valedipalo/miniforge3/lib/python3.9/site-packages (from openai==1.3.7) (1.2.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Users/valedipalo/miniforge3/lib/python3.9/site-packages (from openai==1.3.7) (1.8.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Users/valedipalo/miniforge3/lib/python3.9/site-packages (from openai==1.3.7) (0.25.1)\n",
      "Collecting dataclasses-json<0.7,>=0.5.7\n",
      "  Downloading dataclasses_json-0.6.4-py3-none-any.whl (28 kB)\n",
      "Collecting aiohttp<4.0.0,>=3.8.3\n",
      "  Downloading aiohttp-3.9.3-cp39-cp39-macosx_11_0_arm64.whl (388 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m388.2/388.2 KB\u001B[0m \u001B[31m13.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: tenacity<9.0.0,>=8.1.0 in /Users/valedipalo/miniforge3/lib/python3.9/site-packages (from langchain) (8.2.3)\n",
      "Collecting jsonpatch<2.0,>=1.33\n",
      "  Using cached jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
      "Collecting langsmith<0.1,>=0.0.83\n",
      "  Downloading langsmith-0.0.86-py3-none-any.whl (54 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m54.4/54.4 KB\u001B[0m \u001B[31m2.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting PyYAML>=5.3\n",
      "  Downloading PyYAML-6.0.1-cp39-cp39-macosx_11_0_arm64.whl (174 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m174.4/174.4 KB\u001B[0m \u001B[31m7.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /Users/valedipalo/miniforge3/lib/python3.9/site-packages (from langchain) (4.0.1)\n",
      "Collecting langchain-community<0.1,>=0.0.17\n",
      "  Downloading langchain_community-0.0.17-py3-none-any.whl (1.6 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1.6/1.6 MB\u001B[0m \u001B[31m25.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0ma \u001B[36m0:00:01\u001B[0m\n",
      "\u001B[?25hCollecting kubernetes>=28.1.0\n",
      "  Downloading kubernetes-29.0.0-py2.py3-none-any.whl (1.6 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1.6/1.6 MB\u001B[0m \u001B[31m28.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m\n",
      "\u001B[?25hCollecting bcrypt>=4.0.1\n",
      "  Downloading bcrypt-4.1.2-cp39-abi3-macosx_10_12_universal2.whl (528 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m528.5/528.5 KB\u001B[0m \u001B[31m16.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting onnxruntime>=1.14.1\n",
      "  Downloading onnxruntime-1.17.0-cp39-cp39-macosx_11_0_universal2.whl (14.8 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m14.8/14.8 MB\u001B[0m \u001B[31m13.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\n",
      "\u001B[?25hCollecting fastapi>=0.95.2\n",
      "  Downloading fastapi-0.109.0-py3-none-any.whl (92 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m92.0/92.0 KB\u001B[0m \u001B[31m3.8 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting opentelemetry-sdk>=1.2.0\n",
      "  Downloading opentelemetry_sdk-1.22.0-py3-none-any.whl (105 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m105.6/105.6 KB\u001B[0m \u001B[31m4.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting build>=1.0.3\n",
      "  Downloading build-1.0.3-py3-none-any.whl (18 kB)\n",
      "Collecting uvicorn[standard]>=0.18.3\n",
      "  Downloading uvicorn-0.27.0.post1-py3-none-any.whl (60 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m60.7/60.7 KB\u001B[0m \u001B[31m2.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting pulsar-client>=3.1.0\n",
      "  Downloading pulsar_client-3.4.0-cp39-cp39-macosx_10_15_universal2.whl (10.9 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m10.9/10.9 MB\u001B[0m \u001B[31m20.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m0:01\u001B[0m\n",
      "\u001B[?25hCollecting mmh3>=4.0.1\n",
      "  Downloading mmh3-4.1.0-cp39-cp39-macosx_11_0_arm64.whl (30 kB)\n",
      "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0\n",
      "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.22.0-py3-none-any.whl (18 kB)\n",
      "Collecting importlib-resources\n",
      "  Using cached importlib_resources-6.1.1-py3-none-any.whl (33 kB)\n",
      "Collecting opentelemetry-api>=1.2.0\n",
      "  Downloading opentelemetry_api-1.22.0-py3-none-any.whl (57 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m57.9/57.9 KB\u001B[0m \u001B[31m2.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting grpcio>=1.58.0\n",
      "  Downloading grpcio-1.60.1-cp39-cp39-macosx_10_10_universal2.whl (9.7 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m9.7/9.7 MB\u001B[0m \u001B[31m35.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\n",
      "\u001B[?25hCollecting tokenizers>=0.13.2\n",
      "  Downloading tokenizers-0.15.1-cp39-cp39-macosx_11_0_arm64.whl (2.5 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m2.5/2.5 MB\u001B[0m \u001B[31m29.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\n",
      "\u001B[?25hCollecting overrides>=7.3.1\n",
      "  Downloading overrides-7.7.0-py3-none-any.whl (17 kB)\n",
      "Collecting posthog>=2.4.0\n",
      "  Downloading posthog-3.3.4-py2.py3-none-any.whl (40 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m40.8/40.8 KB\u001B[0m \u001B[31m1.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting opentelemetry-instrumentation-fastapi>=0.41b0\n",
      "  Downloading opentelemetry_instrumentation_fastapi-0.43b0-py3-none-any.whl (11 kB)\n",
      "Collecting chroma-hnswlib==0.7.3\n",
      "  Downloading chroma_hnswlib-0.7.3-cp39-cp39-macosx_11_0_arm64.whl (197 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m197.2/197.2 KB\u001B[0m \u001B[31m7.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting typer>=0.9.0\n",
      "  Using cached typer-0.9.0-py3-none-any.whl (45 kB)\n",
      "Collecting pypika>=0.48.9\n",
      "  Using cached PyPika-0.48.9.tar.gz (67 kB)\n",
      "  Installing build dependencies ... \u001B[?25ldone\n",
      "\u001B[?25h  Getting requirements to build wheel ... \u001B[?25ldone\n",
      "\u001B[?25h  Preparing metadata (pyproject.toml) ... \u001B[?25ldone\n",
      "\u001B[?25hRequirement already satisfied: types-requests<3.0.0.0,>=2.31.0.2 in /Users/valedipalo/miniforge3/lib/python3.9/site-packages (from langchainhub) (2.31.0.6)\n",
      "Requirement already satisfied: beautifulsoup4 in /Users/valedipalo/miniforge3/lib/python3.9/site-packages (from bs4) (4.11.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/valedipalo/miniforge3/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.2.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/valedipalo/miniforge3/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/valedipalo/miniforge3/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (5.2.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/valedipalo/miniforge3/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.8.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/valedipalo/miniforge3/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (21.4.0)\n",
      "Collecting Mako\n",
      "  Downloading Mako-1.3.2-py3-none-any.whl (78 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m78.7/78.7 KB\u001B[0m \u001B[31m3.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: idna>=2.8 in /Users/valedipalo/miniforge3/lib/python3.9/site-packages (from anyio<4,>=3.5.0->openai==1.3.7) (3.3)\n",
      "Collecting pyproject_hooks\n",
      "  Downloading pyproject_hooks-1.0.0-py3-none-any.whl (9.3 kB)\n",
      "Requirement already satisfied: packaging>=19.0 in /Users/valedipalo/miniforge3/lib/python3.9/site-packages (from build>=1.0.3->chromadb) (21.3)\n",
      "Requirement already satisfied: importlib-metadata>=4.6 in /Users/valedipalo/miniforge3/lib/python3.9/site-packages (from build>=1.0.3->chromadb) (4.11.3)\n",
      "Collecting tomli>=1.1.0\n",
      "  Using cached tomli-2.0.1-py3-none-any.whl (12 kB)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0\n",
      "  Downloading marshmallow-3.20.2-py3-none-any.whl (49 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m49.4/49.4 KB\u001B[0m \u001B[31m1.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting starlette<0.36.0,>=0.35.0\n",
      "  Downloading starlette-0.35.1-py3-none-any.whl (71 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m71.1/71.1 KB\u001B[0m \u001B[31m2.8 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: certifi in /Users/valedipalo/miniforge3/lib/python3.9/site-packages (from httpx<1,>=0.23.0->openai==1.3.7) (2022.9.14)\n",
      "Requirement already satisfied: httpcore in /Users/valedipalo/miniforge3/lib/python3.9/site-packages (from httpx<1,>=0.23.0->openai==1.3.7) (1.0.2)\n",
      "Collecting jsonpointer>=1.9\n",
      "  Using cached jsonpointer-2.4-py2.py3-none-any.whl (7.8 kB)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /Users/valedipalo/miniforge3/lib/python3.9/site-packages (from kubernetes>=28.1.0->chromadb) (1.3.2)\n",
      "Requirement already satisfied: urllib3>=1.24.2 in /Users/valedipalo/miniforge3/lib/python3.9/site-packages (from kubernetes>=28.1.0->chromadb) (1.26.11)\n",
      "Requirement already satisfied: requests-oauthlib in /Users/valedipalo/miniforge3/lib/python3.9/site-packages (from kubernetes>=28.1.0->chromadb) (1.3.0)\n",
      "Requirement already satisfied: six>=1.9.0 in /Users/valedipalo/miniforge3/lib/python3.9/site-packages (from kubernetes>=28.1.0->chromadb) (1.15.0)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in /Users/valedipalo/miniforge3/lib/python3.9/site-packages (from kubernetes>=28.1.0->chromadb) (2.8.2)\n",
      "Collecting oauthlib>=3.2.2\n",
      "  Using cached oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "Requirement already satisfied: google-auth>=1.0.1 in /Users/valedipalo/miniforge3/lib/python3.9/site-packages (from kubernetes>=28.1.0->chromadb) (2.6.0)\n",
      "Collecting packaging>=19.0\n",
      "  Using cached packaging-23.2-py3-none-any.whl (53 kB)\n",
      "Requirement already satisfied: flatbuffers in /Users/valedipalo/miniforge3/lib/python3.9/site-packages (from onnxruntime>=1.14.1->chromadb) (22.9.24)\n",
      "Collecting coloredlogs\n",
      "  Using cached coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
      "Requirement already satisfied: sympy in /Users/valedipalo/miniforge3/lib/python3.9/site-packages (from onnxruntime>=1.14.1->chromadb) (1.12)\n",
      "Requirement already satisfied: protobuf in /Users/valedipalo/miniforge3/lib/python3.9/site-packages (from onnxruntime>=1.14.1->chromadb) (3.20.1)\n",
      "Collecting deprecated>=1.2.6\n",
      "  Using cached Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n",
      "Collecting importlib-metadata>=4.6\n",
      "  Downloading importlib_metadata-6.11.0-py3-none-any.whl (23 kB)\n",
      "Collecting googleapis-common-protos~=1.52\n",
      "  Downloading googleapis_common_protos-1.62.0-py2.py3-none-any.whl (228 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m228.7/228.7 KB\u001B[0m \u001B[31m8.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting backoff<3.0.0,>=1.10.0\n",
      "  Using cached backoff-2.2.1-py3-none-any.whl (15 kB)\n",
      "Collecting opentelemetry-proto==1.22.0\n",
      "  Downloading opentelemetry_proto-1.22.0-py3-none-any.whl (50 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m50.8/50.8 KB\u001B[0m \u001B[31m2.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting opentelemetry-exporter-otlp-proto-common==1.22.0\n",
      "  Downloading opentelemetry_exporter_otlp_proto_common-1.22.0-py3-none-any.whl (17 kB)\n",
      "Collecting opentelemetry-semantic-conventions==0.43b0\n",
      "  Downloading opentelemetry_semantic_conventions-0.43b0-py3-none-any.whl (36 kB)\n",
      "Collecting opentelemetry-util-http==0.43b0\n",
      "  Downloading opentelemetry_util_http-0.43b0-py3-none-any.whl (6.9 kB)\n",
      "Collecting opentelemetry-instrumentation==0.43b0\n",
      "  Downloading opentelemetry_instrumentation-0.43b0-py3-none-any.whl (28 kB)\n",
      "Collecting opentelemetry-instrumentation-asgi==0.43b0\n",
      "  Downloading opentelemetry_instrumentation_asgi-0.43b0-py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: setuptools>=16.0 in /Users/valedipalo/miniforge3/lib/python3.9/site-packages (from opentelemetry-instrumentation==0.43b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (63.4.1)\n",
      "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in /Users/valedipalo/miniforge3/lib/python3.9/site-packages (from opentelemetry-instrumentation==0.43b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (1.12.1)\n",
      "Collecting asgiref~=3.0\n",
      "  Using cached asgiref-3.7.2-py3-none-any.whl (24 kB)\n",
      "Collecting monotonic>=1.5\n",
      "  Using cached monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
      "Collecting pydantic-core==2.16.1\n",
      "  Downloading pydantic_core-2.16.1-cp39-cp39-macosx_11_0_arm64.whl (1.8 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1.8/1.8 MB\u001B[0m \u001B[31m28.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0ma \u001B[36m0:00:01\u001B[0m\n",
      "\u001B[?25hCollecting annotated-types>=0.4.0\n",
      "  Downloading annotated_types-0.6.0-py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/valedipalo/miniforge3/lib/python3.9/site-packages (from requests>=2.31.0->trulens_eval==0.21.0) (2.0.4)\n",
      "Requirement already satisfied: validators<1,>=0.2 in /Users/valedipalo/miniforge3/lib/python3.9/site-packages (from streamlit>=1.27.0->trulens_eval==0.21.0) (0.22.0)\n",
      "Requirement already satisfied: toml<2,>=0.10.1 in /Users/valedipalo/miniforge3/lib/python3.9/site-packages (from streamlit>=1.27.0->trulens_eval==0.21.0) (0.10.2)\n",
      "Requirement already satisfied: tzlocal<6,>=1.1 in /Users/valedipalo/miniforge3/lib/python3.9/site-packages (from streamlit>=1.27.0->trulens_eval==0.21.0) (5.2)\n",
      "Requirement already satisfied: click<9,>=7.0 in /Users/valedipalo/miniforge3/lib/python3.9/site-packages (from streamlit>=1.27.0->trulens_eval==0.21.0) (8.1.3)\n",
      "Requirement already satisfied: altair<6,>=4.0 in /Users/valedipalo/miniforge3/lib/python3.9/site-packages (from streamlit>=1.27.0->trulens_eval==0.21.0) (5.2.0)\n",
      "Requirement already satisfied: tornado<7,>=6.0.3 in /Users/valedipalo/miniforge3/lib/python3.9/site-packages (from streamlit>=1.27.0->trulens_eval==0.21.0) (6.2)\n",
      "Requirement already satisfied: pydeck<1,>=0.8.0b4 in /Users/valedipalo/miniforge3/lib/python3.9/site-packages (from streamlit>=1.27.0->trulens_eval==0.21.0) (0.8.1b0)\n",
      "Requirement already satisfied: blinker<2,>=1.0.0 in /Users/valedipalo/miniforge3/lib/python3.9/site-packages (from streamlit>=1.27.0->trulens_eval==0.21.0) (1.4)\n",
      "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /Users/valedipalo/miniforge3/lib/python3.9/site-packages (from streamlit>=1.27.0->trulens_eval==0.21.0) (3.1.40)\n",
      "Requirement already satisfied: pyarrow>=6.0 in /Users/valedipalo/miniforge3/lib/python3.9/site-packages (from streamlit>=1.27.0->trulens_eval==0.21.0) (14.0.2)\n",
      "Requirement already satisfied: cachetools<6,>=4.0 in /Users/valedipalo/miniforge3/lib/python3.9/site-packages (from streamlit>=1.27.0->trulens_eval==0.21.0) (4.2.2)\n",
      "Requirement already satisfied: pandas<3,>=1.3.0 in /Users/valedipalo/miniforge3/lib/python3.9/site-packages (from streamlit>=1.27.0->trulens_eval==0.21.0) (1.4.3)\n",
      "Requirement already satisfied: rich<14,>=10.14.0 in /Users/valedipalo/miniforge3/lib/python3.9/site-packages (from streamlit>=1.27.0->trulens_eval==0.21.0) (13.7.0)\n",
      "Requirement already satisfied: pillow<11,>=7.1.0 in /Users/valedipalo/miniforge3/lib/python3.9/site-packages (from streamlit>=1.27.0->trulens_eval==0.21.0) (9.1.0)\n",
      "Collecting python-decouple<4.0,>=3.6\n",
      "  Downloading python_decouple-3.8-py3-none-any.whl (9.9 kB)\n",
      "Collecting st-annotated-text>=3.0.0\n",
      "  Downloading st_annotated_text-4.0.1-py3-none-any.whl (9.0 kB)\n",
      "Collecting streamlit-faker>=0.0.2\n",
      "  Downloading streamlit_faker-0.0.3-py3-none-any.whl (14 kB)\n",
      "Collecting htbuilder>=0.6.2\n",
      "  Downloading htbuilder-0.6.2-py3-none-any.whl (12 kB)\n",
      "Collecting streamlit-camera-input-live>=0.2.0\n",
      "  Downloading streamlit_camera_input_live-0.2.0-py3-none-any.whl (6.6 kB)\n",
      "Collecting streamlit-keyup>=0.1.9\n",
      "  Downloading streamlit_keyup-0.2.2-py3-none-any.whl (7.4 kB)\n",
      "Collecting streamlit-toggle-switch>=1.0.2\n",
      "  Downloading streamlit_toggle_switch-1.0.2-py3-none-any.whl (635 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m635.4/635.4 KB\u001B[0m \u001B[31m18.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting streamlit-image-coordinates<0.2.0,>=0.1.1\n",
      "  Downloading streamlit_image_coordinates-0.1.6-py3-none-any.whl (6.3 kB)\n",
      "Collecting markdownlit>=0.0.5\n",
      "  Downloading markdownlit-0.0.7-py3-none-any.whl (15 kB)\n",
      "Collecting streamlit-card>=0.0.4\n",
      "  Downloading streamlit_card-1.0.0-py3-none-any.whl (680 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m680.8/680.8 KB\u001B[0m \u001B[31m21.8 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting streamlit-embedcode>=0.1.2\n",
      "  Downloading streamlit_embedcode-0.1.2-py3-none-any.whl (3.5 kB)\n",
      "Collecting streamlit-vertical-slider>=1.0.2\n",
      "  Downloading streamlit_vertical_slider-2.5.5-py3-none-any.whl (1.8 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1.8/1.8 MB\u001B[0m \u001B[31m27.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0ma \u001B[36m0:00:01\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: entrypoints>=0.4 in /Users/valedipalo/miniforge3/lib/python3.9/site-packages (from streamlit-extras>=0.2.7->trulens_eval==0.21.0) (0.4)\n",
      "Collecting huggingface_hub<1.0,>=0.16.4\n",
      "  Downloading huggingface_hub-0.20.3-py3-none-any.whl (330 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m330.1/330.1 KB\u001B[0m \u001B[31m11.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: types-urllib3 in /Users/valedipalo/miniforge3/lib/python3.9/site-packages (from types-requests<3.0.0.0,>=2.31.0.2->langchainhub) (1.26.25.14)\n",
      "Collecting mypy-extensions>=0.3.0\n",
      "  Using cached mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
      "Requirement already satisfied: h11>=0.8 in /Users/valedipalo/miniforge3/lib/python3.9/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.13.0)\n",
      "Collecting uvloop!=0.15.0,!=0.15.1,>=0.14.0\n",
      "  Downloading uvloop-0.19.0-cp39-cp39-macosx_10_9_universal2.whl (1.5 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1.5/1.5 MB\u001B[0m \u001B[31m40.8 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m\n",
      "\u001B[?25hCollecting websockets>=10.4\n",
      "  Downloading websockets-12.0-cp39-cp39-macosx_11_0_arm64.whl (121 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m121.3/121.3 KB\u001B[0m \u001B[31m4.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting httptools>=0.5.0\n",
      "  Downloading httptools-0.6.1-cp39-cp39-macosx_10_9_universal2.whl (152 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m152.9/152.9 KB\u001B[0m \u001B[31m6.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting watchfiles>=0.13\n",
      "  Downloading watchfiles-0.21.0-cp39-cp39-macosx_11_0_arm64.whl (418 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m418.7/418.7 KB\u001B[0m \u001B[31m13.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: soupsieve>1.2 in /Users/valedipalo/miniforge3/lib/python3.9/site-packages (from beautifulsoup4->bs4) (2.3.2.post1)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /Users/valedipalo/miniforge3/lib/python3.9/site-packages (from importlib-resources->chromadb) (3.8.0)\n",
      "Requirement already satisfied: toolz in /Users/valedipalo/miniforge3/lib/python3.9/site-packages (from altair<6,>=4.0->streamlit>=1.27.0->trulens_eval==0.21.0) (0.11.2)\n",
      "Requirement already satisfied: jsonschema>=3.0 in /Users/valedipalo/miniforge3/lib/python3.9/site-packages (from altair<6,>=4.0->streamlit>=1.27.0->trulens_eval==0.21.0) (4.16.0)\n",
      "Requirement already satisfied: jinja2 in /Users/valedipalo/miniforge3/lib/python3.9/site-packages (from altair<6,>=4.0->streamlit>=1.27.0->trulens_eval==0.21.0) (3.1.1)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /Users/valedipalo/miniforge3/lib/python3.9/site-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit>=1.27.0->trulens_eval==0.21.0) (4.0.11)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /Users/valedipalo/miniforge3/lib/python3.9/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.7.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Users/valedipalo/miniforge3/lib/python3.9/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.2.8)\n",
      "Collecting protobuf\n",
      "  Downloading protobuf-4.25.2-cp37-abi3-macosx_10_9_universal2.whl (394 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m394.2/394.2 KB\u001B[0m \u001B[31m20.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: more-itertools in /Users/valedipalo/miniforge3/lib/python3.9/site-packages (from htbuilder>=0.6.2->streamlit-extras>=0.2.7->trulens_eval==0.21.0) (10.1.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/valedipalo/miniforge3/lib/python3.9/site-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (2023.12.1)\n",
      "Requirement already satisfied: filelock in /Users/valedipalo/miniforge3/lib/python3.9/site-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (3.8.0)\n",
      "Requirement already satisfied: markdown in /Users/valedipalo/miniforge3/lib/python3.9/site-packages (from markdownlit>=0.0.5->streamlit-extras>=0.2.7->trulens_eval==0.21.0) (3.3.4)\n",
      "Collecting favicon\n",
      "  Downloading favicon-0.7.0-py2.py3-none-any.whl (5.9 kB)\n",
      "Collecting lxml\n",
      "  Downloading lxml-5.1.0-cp39-cp39-macosx_11_0_arm64.whl (4.5 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m4.5/4.5 MB\u001B[0m \u001B[31m36.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\n",
      "\u001B[?25hCollecting pymdown-extensions\n",
      "  Downloading pymdown_extensions-10.7-py3-none-any.whl (250 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m250.8/250.8 KB\u001B[0m \u001B[31m7.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: pytz>=2020.1 in /Users/valedipalo/miniforge3/lib/python3.9/site-packages (from pandas<3,>=1.3.0->streamlit>=1.27.0->trulens_eval==0.21.0) (2022.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /Users/valedipalo/miniforge3/lib/python3.9/site-packages (from rich<14,>=10.14.0->streamlit>=1.27.0->trulens_eval==0.21.0) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/valedipalo/miniforge3/lib/python3.9/site-packages (from rich<14,>=10.14.0->streamlit>=1.27.0->trulens_eval==0.21.0) (2.17.2)\n",
      "Collecting faker\n",
      "  Downloading Faker-22.6.0-py3-none-any.whl (1.7 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1.7/1.7 MB\u001B[0m \u001B[31m29.8 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: matplotlib in /Users/valedipalo/miniforge3/lib/python3.9/site-packages (from streamlit-faker>=0.0.2->streamlit-extras>=0.2.7->trulens_eval==0.21.0) (3.5.2)\n",
      "Collecting humanfriendly>=9.1\n",
      "  Using cached humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in /Users/valedipalo/miniforge3/lib/python3.9/site-packages (from Mako->alembic>=1.11.2->trulens_eval==0.21.0) (2.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/valedipalo/miniforge3/lib/python3.9/site-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /Users/valedipalo/miniforge3/lib/python3.9/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit>=1.27.0->trulens_eval==0.21.0) (5.0.1)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /Users/valedipalo/miniforge3/lib/python3.9/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit>=1.27.0->trulens_eval==0.21.0) (0.18.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /Users/valedipalo/miniforge3/lib/python3.9/site-packages (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->streamlit>=1.27.0->trulens_eval==0.21.0) (0.1.2)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /Users/valedipalo/miniforge3/lib/python3.9/site-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.8)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /Users/valedipalo/miniforge3/lib/python3.9/site-packages (from matplotlib->streamlit-faker>=0.0.2->streamlit-extras>=0.2.7->trulens_eval==0.21.0) (3.0.9)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/valedipalo/miniforge3/lib/python3.9/site-packages (from matplotlib->streamlit-faker>=0.0.2->streamlit-extras>=0.2.7->trulens_eval==0.21.0) (1.4.2)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/valedipalo/miniforge3/lib/python3.9/site-packages (from matplotlib->streamlit-faker>=0.0.2->streamlit-extras>=0.2.7->trulens_eval==0.21.0) (4.33.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/valedipalo/miniforge3/lib/python3.9/site-packages (from matplotlib->streamlit-faker>=0.0.2->streamlit-extras>=0.2.7->trulens_eval==0.21.0) (0.11.0)\n",
      "Collecting markdown\n",
      "  Downloading Markdown-3.5.2-py3-none-any.whl (103 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m103.9/103.9 KB\u001B[0m \u001B[31m4.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hBuilding wheels for collected packages: millify, pypika\n",
      "  Building wheel for millify (setup.py) ... \u001B[?25ldone\n",
      "\u001B[?25h  Created wheel for millify: filename=millify-0.1.1-py3-none-any.whl size=1844 sha256=2d763888d30d2a97c9286695531b3e330c77ad2798d015fef45f08cff44574af\n",
      "  Stored in directory: /Users/valedipalo/Library/Caches/pip/wheels/4d/a6/58/ab31aca0c3bb6be6ae878845aed85d55c3d580b6d0b2e71486\n",
      "  Building wheel for pypika (pyproject.toml) ... \u001B[?25ldone\n",
      "\u001B[?25h  Created wheel for pypika: filename=PyPika-0.48.9-py2.py3-none-any.whl size=53723 sha256=d2166dce36a244d43f28600738a4215245b09c615cc653e0fa31a207ed8dcd96\n",
      "  Stored in directory: /Users/valedipalo/Library/Caches/pip/wheels/f7/02/64/d541eac67ec459309d1fb19e727f58ecf7ffb4a8bf42d4cfe5\n",
      "Successfully built millify pypika\n",
      "Installing collected packages: python-decouple, pypika, monotonic, mmh3, millify, websockets, uvloop, uvicorn, typer, tqdm, tomli, sqlalchemy, requests, PyYAML, python-dotenv, pydantic-core, pulsar-client, protobuf, packaging, overrides, opentelemetry-util-http, opentelemetry-semantic-conventions, oauthlib, mypy-extensions, munch, merkle-json, Mako, lxml, jsonpointer, importlib-resources, importlib-metadata, humanize, humanfriendly, httptools, htbuilder, grpcio, frozendict, dill, deprecated, chroma-hnswlib, bcrypt, backoff, asgiref, annotated-types, watchfiles, typing-inspect, starlette, st-annotated-text, pyproject_hooks, pydantic, posthog, opentelemetry-proto, opentelemetry-api, marshmallow, markdown, jsonpatch, huggingface_hub, googleapis-common-protos, favicon, faker, coloredlogs, bs4, alembic, aiohttp, tokenizers, pymdown-extensions, opentelemetry-sdk, opentelemetry-instrumentation, opentelemetry-exporter-otlp-proto-common, openai, onnxruntime, langsmith, kubernetes, fastapi, dataclasses-json, build, opentelemetry-instrumentation-asgi, opentelemetry-exporter-otlp-proto-grpc, langchain-core, streamlit-vertical-slider, streamlit-toggle-switch, streamlit-keyup, streamlit-image-coordinates, streamlit-embedcode, streamlit-card, streamlit-camera-input-live, streamlit-aggrid, opentelemetry-instrumentation-fastapi, langchain-community, langchain, chromadb, streamlit-faker, markdownlit, streamlit-extras, trulens_eval\n",
      "  Attempting uninstall: typer\n",
      "    Found existing installation: typer 0.4.2\n",
      "    Uninstalling typer-0.4.2:\n",
      "      Successfully uninstalled typer-0.4.2\n",
      "  Attempting uninstall: tqdm\n",
      "    Found existing installation: tqdm 4.64.1\n",
      "    Uninstalling tqdm-4.64.1:\n",
      "      Successfully uninstalled tqdm-4.64.1\n",
      "  Attempting uninstall: requests\n",
      "    Found existing installation: requests 2.28.1\n",
      "    Uninstalling requests-2.28.1:\n",
      "      Successfully uninstalled requests-2.28.1\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 3.20.1\n",
      "    Uninstalling protobuf-3.20.1:\n",
      "      Successfully uninstalled protobuf-3.20.1\n",
      "  Attempting uninstall: packaging\n",
      "    Found existing installation: packaging 21.3\n",
      "    Uninstalling packaging-21.3:\n",
      "      Successfully uninstalled packaging-21.3\n",
      "  Attempting uninstall: oauthlib\n",
      "    Found existing installation: oauthlib 3.2.0\n",
      "    Uninstalling oauthlib-3.2.0:\n",
      "      Successfully uninstalled oauthlib-3.2.0\n",
      "  Attempting uninstall: importlib-metadata\n",
      "    Found existing installation: importlib-metadata 4.11.3\n",
      "    Uninstalling importlib-metadata-4.11.3:\n",
      "      Successfully uninstalled importlib-metadata-4.11.3\n",
      "  Attempting uninstall: grpcio\n",
      "    Found existing installation: grpcio 1.42.0\n",
      "    Uninstalling grpcio-1.42.0:\n",
      "      Successfully uninstalled grpcio-1.42.0\n",
      "  Attempting uninstall: pydantic\n",
      "    Found existing installation: pydantic 1.9.2\n",
      "    Uninstalling pydantic-1.9.2:\n",
      "      Successfully uninstalled pydantic-1.9.2\n",
      "  Attempting uninstall: markdown\n",
      "    Found existing installation: Markdown 3.3.4\n",
      "    Uninstalling Markdown-3.3.4:\n",
      "      Successfully uninstalled Markdown-3.3.4\n",
      "  Attempting uninstall: aiohttp\n",
      "    Found existing installation: aiohttp 3.8.1\n",
      "    Uninstalling aiohttp-3.8.1:\n",
      "      Successfully uninstalled aiohttp-3.8.1\n",
      "  Attempting uninstall: openai\n",
      "    Found existing installation: openai 1.3.5\n",
      "    Uninstalling openai-1.3.5:\n",
      "      Successfully uninstalled openai-1.3.5\n",
      "\u001B[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "thinc 8.1.2 requires pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4, but you have pydantic 2.6.0 which is incompatible.\n",
      "tensorflow-macos 2.10.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.25.2 which is incompatible.\n",
      "tensorboard 2.10.1 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.25.2 which is incompatible.\n",
      "spacy 3.4.1 requires pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4, but you have pydantic 2.6.0 which is incompatible.\n",
      "spacy 3.4.1 requires typer<0.5.0,>=0.3.0, but you have typer 0.9.0 which is incompatible.\n",
      "confection 0.0.1 requires pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4, but you have pydantic 2.6.0 which is incompatible.\u001B[0m\u001B[31m\n",
      "\u001B[0mSuccessfully installed Mako-1.3.2 PyYAML-6.0.1 aiohttp-3.9.3 alembic-1.13.1 annotated-types-0.6.0 asgiref-3.7.2 backoff-2.2.1 bcrypt-4.1.2 bs4-0.0.2 build-1.0.3 chroma-hnswlib-0.7.3 chromadb-0.4.22 coloredlogs-15.0.1 dataclasses-json-0.6.4 deprecated-1.2.14 dill-0.3.8 faker-22.6.0 fastapi-0.109.0 favicon-0.7.0 frozendict-2.4.0 googleapis-common-protos-1.62.0 grpcio-1.60.1 htbuilder-0.6.2 httptools-0.6.1 huggingface_hub-0.20.3 humanfriendly-10.0 humanize-4.9.0 importlib-metadata-6.11.0 importlib-resources-6.1.1 jsonpatch-1.33 jsonpointer-2.4 kubernetes-29.0.0 langchain-0.1.5 langchain-community-0.0.17 langchain-core-0.1.18 langsmith-0.0.86 lxml-5.1.0 markdown-3.5.2 markdownlit-0.0.7 marshmallow-3.20.2 merkle-json-1.0.0 millify-0.1.1 mmh3-4.1.0 monotonic-1.6 munch-4.0.0 mypy-extensions-1.0.0 oauthlib-3.2.2 onnxruntime-1.17.0 openai-1.3.7 opentelemetry-api-1.22.0 opentelemetry-exporter-otlp-proto-common-1.22.0 opentelemetry-exporter-otlp-proto-grpc-1.22.0 opentelemetry-instrumentation-0.43b0 opentelemetry-instrumentation-asgi-0.43b0 opentelemetry-instrumentation-fastapi-0.43b0 opentelemetry-proto-1.22.0 opentelemetry-sdk-1.22.0 opentelemetry-semantic-conventions-0.43b0 opentelemetry-util-http-0.43b0 overrides-7.7.0 packaging-23.2 posthog-3.3.4 protobuf-4.25.2 pulsar-client-3.4.0 pydantic-2.6.0 pydantic-core-2.16.1 pymdown-extensions-10.7 pypika-0.48.9 pyproject_hooks-1.0.0 python-decouple-3.8 python-dotenv-1.0.1 requests-2.31.0 sqlalchemy-2.0.25 st-annotated-text-4.0.1 starlette-0.35.1 streamlit-aggrid-0.3.4.post3 streamlit-camera-input-live-0.2.0 streamlit-card-1.0.0 streamlit-embedcode-0.1.2 streamlit-extras-0.3.6 streamlit-faker-0.0.3 streamlit-image-coordinates-0.1.6 streamlit-keyup-0.2.2 streamlit-toggle-switch-1.0.2 streamlit-vertical-slider-2.5.5 tokenizers-0.15.1 tomli-2.0.1 tqdm-4.66.1 trulens_eval-0.21.0 typer-0.9.0 typing-inspect-0.9.0 uvicorn-0.27.0.post1 uvloop-0.19.0 watchfiles-0.21.0 websockets-12.0\n"
     ]
    }
   ],
   "source": [
    "! pip install trulens_eval==0.21.0 openai==1.3.7 langchain chromadb langchainhub bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "2wbCi0Fknb4A",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"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\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uMOXx69lnb4A",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Import from LangChain and TruLens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "75g8trfCnb4A",
    "outputId": "7c438c0f-fcb6-4715-bd26-5fe0c2746b2c",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🦑 Tru initialized with db url sqlite:///default.sqlite .\n",
      "🛑 Secret keys may be written to the database. See the `database_redact_keys` option of `Tru` to prevent this.\n"
     ]
    }
   ],
   "source": [
    "# Imports main tools:\n",
    "from trulens_eval import TruChain, Feedback, Huggingface, Tru\n",
    "from trulens_eval.schema import FeedbackResult\n",
    "tru = Tru()\n",
    "tru.reset_database()\n",
    "\n",
    "# Imports from langchain to build app\n",
    "import bs4\n",
    "from langchain import hub\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.document_loaders import WebBaseLoader\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.schema import StrOutputParser\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain_core.runnables import RunnablePassthrough"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XC2r9wTBnb4A",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Load documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from langchain.document_loaders import UnstructuredMarkdownLoader\n",
    "from langchain.text_splitter import MarkdownHeaderTextSplitter\n",
    "#DOCUMENT LOADING\n",
    "file_path = \"../../Data/Scraping_Bocconi_converted_no_dup_check.md\"\n",
    "with open(file_path, 'r') as file:\n",
    "    markdown_content = file.read()\n",
    "\n",
    "#CREATE VECTOR STORE\n",
    "headers_to_split_on = [\n",
    "    (\"#\", \"Header 1\"),\n",
    "    (\"##\", \"Header 2\"),\n",
    "    (\"###\", \"Header 3\"),\n",
    "    (\"####\", \"Header 4\"),]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oMcGDGn6nb4B",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Create Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "markdown_splitter = MarkdownHeaderTextSplitter(\n",
    "    headers_to_split_on=headers_to_split_on)\n",
    "splits = markdown_splitter.split_text(markdown_content)\n",
    "vectorstore = Chroma.from_documents(documents=splits, embedding=OpenAIEmbeddings())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vJE8lB6gnb4B",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Create RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g_P02Sehnb4B",
    "outputId": "8b656b16-0061-48c0-c620-4c818acc75df",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kf4vPHkApv6H",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### VDP - Create your own RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def pretty_print_docs(docs):\n",
    "    print(f\"\\n{'-' * 100}\\n\".join([f\"Document {i+1}:\\n\\n\" + d.page_content for i, d in enumerate(docs)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "qsJ9DsjNpuaP",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from langchain.retrievers.document_compressors import LLMChainExtractor\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "\n",
    "compressor = LLMChainExtractor.from_llm(llm)\n",
    "\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor,\n",
    "    base_retriever=vectorstore.as_retriever()\n",
    ")\n",
    "\n",
    "rag_chain_compressed = (\n",
    "    {\"context\": compression_retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "### Selfquery retriever - https://python.langchain.com/docs/modules/data_connection/retrievers/self_query/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain_openai\n",
      "  Downloading langchain_openai-0.0.5-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting langchain-core<0.2,>=0.1.16 (from langchain_openai)\n",
      "  Downloading langchain_core-0.1.18-py3-none-any.whl.metadata (6.0 kB)\n",
      "Requirement already satisfied: numpy<2,>=1 in /Users/valedipalo/miniforge3/envs/aienv/lib/python3.10/site-packages (from langchain_openai) (1.26.2)\n",
      "Collecting openai<2.0.0,>=1.10.0 (from langchain_openai)\n",
      "  Downloading openai-1.11.0-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting tiktoken<0.6.0,>=0.5.2 (from langchain_openai)\n",
      "  Downloading tiktoken-0.5.2-cp310-cp310-macosx_11_0_arm64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /Users/valedipalo/miniforge3/envs/aienv/lib/python3.10/site-packages (from langchain-core<0.2,>=0.1.16->langchain_openai) (6.0.1)\n",
      "Requirement already satisfied: anyio<5,>=3 in /Users/valedipalo/miniforge3/envs/aienv/lib/python3.10/site-packages (from langchain-core<0.2,>=0.1.16->langchain_openai) (3.7.1)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /Users/valedipalo/miniforge3/envs/aienv/lib/python3.10/site-packages (from langchain-core<0.2,>=0.1.16->langchain_openai) (1.33)\n",
      "Collecting langsmith<0.1,>=0.0.83 (from langchain-core<0.2,>=0.1.16->langchain_openai)\n",
      "  Downloading langsmith-0.0.86-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: packaging<24.0,>=23.2 in /Users/valedipalo/.local/lib/python3.10/site-packages (from langchain-core<0.2,>=0.1.16->langchain_openai) (23.2)\n",
      "Requirement already satisfied: pydantic<3,>=1 in /Users/valedipalo/miniforge3/envs/aienv/lib/python3.10/site-packages (from langchain-core<0.2,>=0.1.16->langchain_openai) (2.5.2)\n",
      "Requirement already satisfied: requests<3,>=2 in /Users/valedipalo/miniforge3/envs/aienv/lib/python3.10/site-packages (from langchain-core<0.2,>=0.1.16->langchain_openai) (2.31.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /Users/valedipalo/miniforge3/envs/aienv/lib/python3.10/site-packages (from langchain-core<0.2,>=0.1.16->langchain_openai) (8.2.3)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Users/valedipalo/miniforge3/envs/aienv/lib/python3.10/site-packages (from openai<2.0.0,>=1.10.0->langchain_openai) (1.8.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Users/valedipalo/miniforge3/envs/aienv/lib/python3.10/site-packages (from openai<2.0.0,>=1.10.0->langchain_openai) (0.25.1)\n",
      "Requirement already satisfied: sniffio in /Users/valedipalo/miniforge3/envs/aienv/lib/python3.10/site-packages (from openai<2.0.0,>=1.10.0->langchain_openai) (1.3.0)\n",
      "Requirement already satisfied: tqdm>4 in /Users/valedipalo/miniforge3/envs/aienv/lib/python3.10/site-packages (from openai<2.0.0,>=1.10.0->langchain_openai) (4.66.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in /Users/valedipalo/miniforge3/envs/aienv/lib/python3.10/site-packages (from openai<2.0.0,>=1.10.0->langchain_openai) (4.8.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /Users/valedipalo/miniforge3/envs/aienv/lib/python3.10/site-packages (from tiktoken<0.6.0,>=0.5.2->langchain_openai) (2023.10.3)\n",
      "Requirement already satisfied: idna>=2.8 in /Users/valedipalo/miniforge3/envs/aienv/lib/python3.10/site-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.16->langchain_openai) (3.4)\n",
      "Requirement already satisfied: exceptiongroup in /Users/valedipalo/.local/lib/python3.10/site-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.16->langchain_openai) (1.2.0)\n",
      "Requirement already satisfied: certifi in /Users/valedipalo/miniforge3/envs/aienv/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.10.0->langchain_openai) (2023.11.17)\n",
      "Requirement already satisfied: httpcore in /Users/valedipalo/miniforge3/envs/aienv/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.10.0->langchain_openai) (1.0.2)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /Users/valedipalo/miniforge3/envs/aienv/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.2,>=0.1.16->langchain_openai) (2.4)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /Users/valedipalo/miniforge3/envs/aienv/lib/python3.10/site-packages (from pydantic<3,>=1->langchain-core<0.2,>=0.1.16->langchain_openai) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.14.5 in /Users/valedipalo/miniforge3/envs/aienv/lib/python3.10/site-packages (from pydantic<3,>=1->langchain-core<0.2,>=0.1.16->langchain_openai) (2.14.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/valedipalo/miniforge3/envs/aienv/lib/python3.10/site-packages (from requests<3,>=2->langchain-core<0.2,>=0.1.16->langchain_openai) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/valedipalo/miniforge3/envs/aienv/lib/python3.10/site-packages (from requests<3,>=2->langchain-core<0.2,>=0.1.16->langchain_openai) (2.1.0)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/valedipalo/miniforge3/envs/aienv/lib/python3.10/site-packages (from httpcore->httpx<1,>=0.23.0->openai<2.0.0,>=1.10.0->langchain_openai) (0.14.0)\n",
      "Downloading langchain_openai-0.0.5-py3-none-any.whl (29 kB)\n",
      "Downloading langchain_core-0.1.18-py3-none-any.whl (237 kB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m237.0/237.0 kB\u001B[0m \u001B[31m4.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m\n",
      "\u001B[?25hDownloading openai-1.11.0-py3-none-any.whl (226 kB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m226.0/226.0 kB\u001B[0m \u001B[31m5.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading tiktoken-0.5.2-cp310-cp310-macosx_11_0_arm64.whl (953 kB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m953.9/953.9 kB\u001B[0m \u001B[31m18.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m\n",
      "\u001B[?25hDownloading langsmith-0.0.86-py3-none-any.whl (54 kB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m54.4/54.4 kB\u001B[0m \u001B[31m2.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hInstalling collected packages: tiktoken, openai, langsmith, langchain-core, langchain_openai\n",
      "  Attempting uninstall: tiktoken\n",
      "    Found existing installation: tiktoken 0.5.1\n",
      "    Uninstalling tiktoken-0.5.1:\n",
      "      Successfully uninstalled tiktoken-0.5.1\n",
      "  Attempting uninstall: openai\n",
      "    Found existing installation: openai 1.3.5\n",
      "    Uninstalling openai-1.3.5:\n",
      "      Successfully uninstalled openai-1.3.5\n",
      "  Attempting uninstall: langsmith\n",
      "    Found existing installation: langsmith 0.0.75\n",
      "    Uninstalling langsmith-0.0.75:\n",
      "      Successfully uninstalled langsmith-0.0.75\n",
      "  Attempting uninstall: langchain-core\n",
      "    Found existing installation: langchain-core 0.1.3\n",
      "    Uninstalling langchain-core-0.1.3:\n",
      "      Successfully uninstalled langchain-core-0.1.3\n",
      "Successfully installed langchain-core-0.1.18 langchain_openai-0.0.5 langsmith-0.0.86 openai-1.11.0 tiktoken-0.5.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install langchain_openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from langchain.retrievers.self_query.base import SelfQueryRetriever\n",
    "from langchain.chains.query_constructor.base import AttributeInfo\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "metadata_field_info = [\n",
    "    AttributeInfo(\n",
    "        name=\"Header 1\",\n",
    "        description=\"a primary category or a general topic. It introduces the broader theme under which more specific information is grouped. In a retrieval task, it acts as the first level of data filtering or organization, offering a broad overview of the context or subject area.\",\n",
    "        type=\"string\",\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"Header 2\",\n",
    "        description=\"This is a subtheme or subcategory of Header 1. It provides a further level of detail, focusing on a specific aspect of the main theme. It serves to refine the search or understanding within the general topic defined by Header 1, guiding the user towards more targeted information.\",\n",
    "        type=\"string\",\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"Header 3\",\n",
    "        description=\"This represents an even more specific subdivision of Header 2. This level may contain rules, guidelines, or particular details concerning the subtheme. In a retrieval task, this header helps to focus on very specific aspects within the subcategory, making the search even more targeted. \",\n",
    "        type=\"string\",\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"Header 4\",\n",
    "        description=\"This is the most specific level, typically formulated as a question or a very precise statement. It serves to direct the user or the retrieval system towards a highly detailed and specific answer or information, often of a practical or operational nature. It's the level that directly responds to the user's questions or needs.\",\n",
    "        type=\"string\",\n",
    "    ),\n",
    "]\n",
    "\n",
    "document_content_description = \"Frequently asked questions\"\n",
    "\n",
    "llm = ChatOpenAI(temperature=0)\n",
    "self_retriever = SelfQueryRetriever.from_llm(\n",
    "    llm,\n",
    "    vectorstore,\n",
    "    document_content_description, #\n",
    "    metadata_field_info,          #\n",
    "    verbose= True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8KtGjXK6nb4C",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Send your first request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "id": "aASL_yIinb4C",
    "outputId": "1a178a47-c16d-45f8-cfcc-e08eecf719e8",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Per fare l'ingresso in residenza, devi accedere al link all'orario di apertura indicato e seguire i passaggi previsti, tra cui l'accesso con le credenziali Bocconi, la selezione della residenza e della tipologia di camera, il salvataggio dei dati e la conferma dei dati. Successivamente, devi caricare i documenti e inoltrare la domanda entro le ore 23:59 del giorno stesso. L'ufficio verificherà la tua prenotazione e ti darà un esito attraverso MyApplication nei giorni successivi.\""
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain.invoke(\"Come posso fare l'ingresso in residenza? \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 174
    },
    "id": "rJBW3zVdqIsc",
    "outputId": "6ebcdb7a-eab8-4c96-e70b-74e846330bf1",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/valedipalo/miniforge3/envs/aienv/lib/python3.10/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n",
      "/Users/valedipalo/miniforge3/envs/aienv/lib/python3.10/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n",
      "/Users/valedipalo/miniforge3/envs/aienv/lib/python3.10/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n",
      "/Users/valedipalo/miniforge3/envs/aienv/lib/python3.10/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Per fare l'ingresso in residenza, devi accedere al link indicato e inserire le tue credenziali Bocconi. Successivamente, seleziona la residenza e la tipologia di camera desiderate, salva i dati e conferma la prenotazione. Dopo aver effettuato l'ingresso, compila la sezione Room check del check-in online per segnalare eventuali anomalie o malfunzionamenti.\""
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain_compressed.invoke(\"Come posso fare l'ingresso in residenza?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/valedipalo/miniforge3/envs/aienv/lib/python3.10/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n",
      "/Users/valedipalo/miniforge3/envs/aienv/lib/python3.10/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n",
      "/Users/valedipalo/miniforge3/envs/aienv/lib/python3.10/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n",
      "/Users/valedipalo/miniforge3/envs/aienv/lib/python3.10/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1:\n",
      "\n",
      "- ACCEDI al link all'orario di apertura indicato: ti troverai in una \"waiting room\" virtuale. Quando arriva il tuo turno, accedi inserendo le credenziali Bocconi (matricola/username e password).\n",
      "- ENTRA NELLA SEZIONE \"Accommodation choice\"\n",
      "- SELEZIONA LA RESIDENZA\n",
      "- SELEZIONA LA TIPOLOGIA DI CAMERA\n",
      "- CLICCA SU \"SAVE\" (salva) in fondo alla sezione\n",
      "- CLICCA SU \"CONFERMA DATI / SUBMIT DATA\" (conferma dati)\n",
      "- CARICA I DOCUMENTI E INOLTRA LA DOMANDA\n",
      "- L'ufficio verificherà che la tua prenotazione sia andata a buon fine e it darà un esito attraverso MyApplication nei giorni successivi.\n",
      "- Se non sei riuscito a cliccare su \"Save\" in fondo alla sezione, significa che i posti disponibili sono terminati. Procedi a selezionare una nuova tipologia di camera e/o Residenza;\n",
      "- Se sei riuscito a selezionare una Residenza e tipologia di camera e a salvare la sezione, ma al momento del \"Submit data\" (conferma dati) un altro utente ha occupato l’ultimo posto disponibile per la Residenza e/o tipologia da te selezionata, il bottone rosso “Submit data” (conferma dati) rimane visibile: questo significa che, anche sei hai già espresso e salvato una preferenza nella sezione \"Residenza e tipologia camera\", la tua prenotazione non è andata a buon fine per mancanza di posti.\n",
      "- Per questo motivo non sei stato in grado di confermare i dati, e dovrai quindi:\n",
      "  - rientrare nella sezione \"Residenza e tipologia camera\";\n",
      "  - procedere a selezionare una nuova opzione;\n",
      "  - salvare nuovamente i dati nella sezione cliccando su “SAVE”;\n",
      "  - riprovare a confermare i dati per la nuova selezione, cliccando sul bottone rosso “Submit data” (conferma dati).\n",
      "- Se nel cercare una nuova opzione da selezionare tutti i posti disponibili fossero già esauriti, il sistema continuerebbe a farti visualizzare, nella sezione \"Residenza e tipologia camera\", la tua selezione originaria: questo NON significa che la prenotazione sia andata a buon fine.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 2:\n",
      "\n",
      "Se non ti sei trovato/a bene nell'alloggio assegnato e vorresti cambiarlo, puoi attendere la fine della tua assegnazione, non presentare domanda di conferma per l'anno accademico successivo e invece presentare domanda di nuova ammissione in una delle possibilità di application previste, in modo tale da poter concorrere per l'assegnazione di un nuovo alloggio.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 3:\n",
      "\n",
      "1. Compila la procedura di check-in online che troverai già \"in corso\" su MyApplication entro la scadenza indicata, che coincide con quella di pagamento della prima rata alloggi. La scadenza per il pagamento/check-in online viene sempre specificata nell'esito  \n",
      "2. Prendi possesso del tuo alloggio presentandoti direttamente alla reception a partire dalla data di inizio assegnazione; ti servirà solo un documento di identità in corso di validità.  \n",
      "3. Dopo aver effettuato il tuo ingresso in residenza, riceverai una email che ti invita a rientrare nella procedura online tramite MyApplication e compilare la sezione Room check del check-in online. E' molto importante che tu compili questa sezione, fornendo una valutazione dell'alloggio al tuo arrivo e segnalando eventuali anomalie o malfunzionamenti al gestore della residenza. Il gestore e gli uffici competenti prenderanno in esame quanto da te riportato e si adopereranno per migliorare il servizio.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 4:\n",
      "\n",
      "Se hai effettuato il recesso ma durante lo stesso anno accademico desideri ottenere una nuova assegnazione in una delle residenze, potrai presentare richiesta in una delle sessioni Open reservation che si svolgeranno durante l'anno.  \n",
      "In questo caso, non sarai tenuto al versamento di un nuovo deposito cauzionale, quello da te versato per la precedente assegnazione dello stesso anno accademico sarà considerato valido.  \n",
      "Se invece vorrai ottenere una nuova assegnazione di alloggio per l’a.a. successivo, dovrai presentare richiesta in una delle possibilità che saranno previste per quell’anno accademico e ti sarà richiesto il versamento di un nuovo deposito cauzionale.\n"
     ]
    }
   ],
   "source": [
    "compressed_docs = compression_retriever.get_relevant_documents(\"Come posso fare l'ingresso in residenza?\")\n",
    "pretty_print_docs(compressed_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Prima di accedere alla domanda prendi visione dei documenti utili (Regolamento Residenze Bocconi a.a. 2023-24 e Informativa privacy) disponibili al seguente link.  \\nTieni a portata di mano le credenziali di accesso, cerca una connessione internet veloce e utilizza un unico dispositivo per accedere alla domanda online nel momento dell’apertura. All’apertura della domanda online, segui tutti i passaggi previsti:  \\n> ACCEDI al link all\\'orario di apertura indicato: ti troverai in una \"waiting room\" virtuale. Quando arriva il tuo turno, accedi inserendo le credenziali Bocconi (matricola/username e password).  \\n> ENTRA NELLA SEZIONE \"Accommodation choice\"  \\n> SELEZIONA LA RESIDENZA  \\nSe non visualizzi alcuna opzione significa che i posti disponibili sono esauriti.  \\n> SELEZIONA LA TIPOLOGIA DI CAMERA  \\nSe non visualizzi alcuna opzione significa che i posti disponibili sono esauriti.  \\n> CLICCA SU \"SAVE\" (salva) in fondo alla sezione  \\nSe non riesci a cliccare significa che i posti disponibili sono esauriti.  \\n> CLICCA SU \"CONFERMA DATI / SUBMIT DATA\" (conferma dati)  \\nSe non riesci a cliccare significa che i posti disponibili sono esauriti.  \\nLa prenotazione è andata a buon fine esclusivamente se il pulsante rosso \"Conferma dati /Submit data\" non è più visibile e al suo posto compare la scritta: \"Hai confermato i dati il gg/mm/aaaa\".  \\n> CARICA I DOCUMENTI E INOLTRA LA DOMANDA  \\nProcedi entro le ore 23:59 del giorno stesso al caricamento di tutti i documenti disponibili in application da stampare, firmare a mano e caricare negli appositi spazi upload e al pagamento del deposito cauzionale. Clicca su \"Inoltra domanda/ Submit application\" entro le ore 23.59.  \\nL\\'ufficio verificherà che la tua prenotazione sia andata a buon fine e it darà un esito attraverso MyApplication nei giorni successivi.  \\nAttenzione  \\nSe non sei riuscito a cliccare su \"Save\" in fondo alla sezione, significa che i posti disponibili sono terminati. Procedi a selezionare una nuova tipologia di camera e/o Residenza;  \\nSe sei riuscito a selezionare una Residenza e tipologia di camera e a salvare la sezione, ma al momento del \"Submit data\" (conferma dati) un altro utente ha occupato l’ultimo posto disponibile per la Residenza e/o tipologia da te selezionata, il bottone rosso “Submit data” (conferma dati) rimane visibile: questo significa che, anche sei hai già espresso e salvato una preferenza nella sezione \"Residenza e tipologia camera\", la tua prenotazione non è andata a buon fine per mancanza di posti.  \\nPer questo motivo non sei stato in grado di confermare i dati, e dovrai quindi:  \\nrientrare nella sezione \"Residenza e tipologia camera\";\\nprocedere a selezionare una nuova opzione;\\nsalvare nuovamente i dati nella sezione cliccando su “SAVE”;\\nriprovare a confermare i dati per la nuova selezione, cliccando sul bottone rosso “Submit data” (conferma dati).\\nSe nel cercare una nuova opzione da selezionare tutti i posti disponibili fossero già esauriti, il sistema continuerebbe a farti visualizzare, nella sezione \"Residenza e tipologia camera\", la tua selezione originaria: questo NON significa che la prenotazione sia andata a buon fine.', metadata={'Header 1': 'Alloggi on campus', 'Header 2': \"Disponibilità di alloggi in corso d'anno\", 'Header 3': 'Open reservation mensili a.a. 2023-30', 'Header 4': 'Come\\xa0presentare\\xa0domanda\\xa0Open reservation mensili'}),\n",
       " Document(page_content=\"Non è possibile richiedere di cambiare stanza o residenza, poiché il numero di posti disponibili nelle residenze è limitato e per questo motivo non sarebbe possibile soddisfare tutte le richieste di questo tipo. Se qualche aspetto tecnico del tuo alloggio non ti soddisfa o se hai necessità di interventi di manutenzione puoi rivolgerti alla gestione che farà tutto il possibile per risolvere eventuali malfunzionamenti o altre problematiche riscontrate.  \\nSuggerimento: Se non ti sei trovato/a bene nell'alloggio assegnato e vorresti cambiarlo, puoi attendere la fine della tua assegnazione, non presentare domanda di conferma per l'anno accademico successivo e invece presentare domanda di nuova ammissione in una delle possibilità di application previste, in modo tale da poter concorrere per l'assegnazione di un nuovo alloggio.\", metadata={'Header 1': 'Alloggi on campus', 'Header 2': 'Vivere le residenze', 'Header 3': 'Posso chiedere di cambiare camera o residenza?', 'Header 4': 'Posso chiedere di cambiare camera o residenza?'}),\n",
       " Document(page_content='Dopo aver prenotato un alloggio e aver ricevuto conferma dell\\'assegnazione da parte dell\\'ufficio, sei pronto per il check-in! Segui gli step indicati di seguito, validi per tutte le assegnazioni alloggio:  \\n1. Consulta l\\'esito della tua assegnazione cliccando sul pulsante \"esito / result\" in MyApplication  \\n2. Procedi al pagamento della prima rata alloggi entro la scadenza indicata nell\\'esito, tramite MAV\\n(che trovi già o troverai in agenda you@B - questa informazione viene specificata nell\\'esito)  \\n3. Compila la procedura di check-in online che troverai già \"in corso\" su MyApplication entro la scadenza indicata, che coincide con quella di pagamento della prima rata alloggi. La scadenza per il pagamento/check-in online viene sempre specificata nell\\'esito  \\n4. Prendi possesso del tuo alloggio presentandoti direttamente alla reception a partire dalla data di inizio assegnazione; ti servirà solo un documento di identità in corso di validità.  \\nRoom check\\nDopo aver effettuato il tuo ingresso in residenza, riceverai una email che ti invita a rientrare nella procedura online tramite MyApplication e compilare la sezione Room check del check-in online. E\\' molto importante che tu compili questa sezione, fornendo una valutazione dell\\'alloggio al tuo arrivo e segnalando eventuali anomalie o malfunzionamenti al gestore della residenza. Il gestore e gli uffici competenti prenderanno in esame quanto da te riportato e si adopereranno per migliorare il servizio.  \\nSuggerimento: si consiglia di effettuare il check-in in residenza preferibilmente tra le ore 9:00 e le ore 17:00, così che lo staff della residenza possa garantire qualsiasi supporto necessario.  \\nCheck in estivo  \\nPer tutti gli studenti che entreranno in residenza a fine agosto si specifica che:\\n>il MAV per il pagamento della prima rata alloggi sarà disponibile in agenda you@B nel corso del mese di luglio. Attenzione: per gli studenti di anni successivi al primo, per poter visualizzare il MAV in agenda è necessario prima completare l\\'iscrizione all\\'anno accademico di riferimento.  \\n> Qualora sia stata caricata la ricevuta di pagamento ma la situazione finanziaria in Agenda yoU@B non sia ancora stata aggiornata (luce verde di fianco al relativo pagamento), sarà possibile comunque effettuare il check-in in residenza.  \\n> N.B. Se sei uno studente ammesso al programma “Una Scelta Possibile”, all’interno dello spazio upload “Ricevuta pagamento prima rata_Payment receipt” carica la \"Domanda alloggio_Housing application\" a titolo gratuito già caricata all\\'interno della tua Special Programs Housing Application.', metadata={'Header 1': 'Alloggi on campus', 'Header 2': 'Check-in e check-out', 'Header 3': 'Quando e come posso effettuare il check-in?', 'Header 4': 'Quando e come posso effettuare il check-in?'}),\n",
       " Document(page_content=\"Se hai effettuato il recesso ma durante lo stesso anno accademico desideri ottenere una nuova assegnazione in una delle residenze, potrai presentare richiesta in una delle sessioni Open reservation che si svolgeranno durante l'anno.  \\nIn questo caso, non sarai tenuto al versamento di un nuovo deposito cauzionale, quello da te versato per la precedente assegnazione dello stesso anno accademico sarà considerato valido.  \\nSe invece vorrai ottenere una nuova assegnazione di alloggio per l’a.a. successivo, dovrai presentare richiesta in una delle possibilità che saranno previste per quell’anno accademico e ti sarà richiesto il versamento di un nuovo deposito cauzionale.\", metadata={'Header 1': 'Alloggi on campus', 'Header 2': 'Recesso e rinuncia', 'Header 3': 'Ho effettuato il recesso al 24 dicembre. Posso ottenere una nuova assegnazione in una delle residenze Bocconi? In questo caso, dovrò versare un nuovo deposito cauzionale?', 'Header 4': 'Ho effettuato il recesso al 24 dicembre. Posso ottenere una nuova assegnazione in una delle residenze Bocconi? In questo caso, dovrò versare un nuovo deposito cauzionale?'})]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "self_retriever.invoke(\" Come posso fare l'ingresso in residenza?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "80Wh8ADNnb4C",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Initialize Feedback Function(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KzyM6Vornb4C",
    "outputId": "91e7afd3-312c-48c2-9a36-5efeab4a9003",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ In groundedness_measure_with_cot_reasons, input source will be set to __record__.app.first.steps.context.first.get_relevant_documents.rets.collect() .\n",
      "✅ In groundedness_measure_with_cot_reasons, input statement will be set to __record__.main_output or `Select.RecordOutput` .\n",
      "✅ In relevance, input prompt will be set to __record__.main_input or `Select.RecordInput` .\n",
      "✅ In relevance, input response will be set to __record__.main_output or `Select.RecordOutput` .\n",
      "✅ In qs_relevance, input question will be set to __record__.main_input or `Select.RecordInput` .\n",
      "✅ In qs_relevance, input statement will be set to __record__.app.first.steps.context.first.get_relevant_documents.rets .\n"
     ]
    }
   ],
   "source": [
    "from trulens_eval.feedback.provider import OpenAI\n",
    "import numpy as np\n",
    "\n",
    "# Initialize provider class\n",
    "openai = OpenAI()\n",
    "\n",
    "# select context to be used in feedback. the location of context is app specific.\n",
    "from trulens_eval.app import App\n",
    "context = App.select_context(rag_chain)\n",
    "\n",
    "from trulens_eval.feedback import Groundedness\n",
    "grounded = Groundedness(groundedness_provider=OpenAI())\n",
    "# Define a groundedness feedback function\n",
    "f_groundedness = (\n",
    "    Feedback(grounded.groundedness_measure_with_cot_reasons)\n",
    "    .on(context.collect()) # collect context chunks into a list\n",
    "    .on_output()\n",
    "    .aggregate(grounded.grounded_statements_aggregator)\n",
    ")\n",
    "\n",
    "# Question/answer relevance between overall question and answer.\n",
    "f_qa_relevance = Feedback(openai.relevance).on_input_output()\n",
    "# Question/statement relevance between question and each context chunk.\n",
    "f_context_relevance = (\n",
    "    Feedback(openai.qs_relevance)\n",
    "    .on_input()\n",
    "    .on(context)\n",
    "    .aggregate(np.mean)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 426
    },
    "id": "L5u8N8Czql4k",
    "outputId": "e97d5b90-586f-45c5-b3fa-9f5d1d8a6d58",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found more than one `BaseRetriever` in app:\n\t<class 'langchain.retrievers.contextual_compression.ContextualCompressionRetriever'> at first.steps.context.first\n\t<class 'langchain_core.vectorstores.VectorStoreRetriever'> at first.steps.context.first.base_retriever",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-27-e6c6e33e1795>\u001B[0m in \u001B[0;36m<cell line: 1>\u001B[0;34m()\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mcontext\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mApp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mselect_context\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mrag_chain_compressed\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      2\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0mtrulens_eval\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfeedback\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mGroundedness\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[0mgrounded\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mGroundedness\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mgroundedness_provider\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mOpenAI\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[0;31m# Define a groundedness feedback function\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.10/dist-packages/trulens_eval/app.py\u001B[0m in \u001B[0;36mselect_context\u001B[0;34m(cls, app)\u001B[0m\n\u001B[1;32m    479\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mtype\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mapp\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__module__\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstartswith\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"langchain\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    480\u001B[0m             \u001B[0;32mfrom\u001B[0m \u001B[0mtrulens_eval\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtru_chain\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mTruChain\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 481\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mTruChain\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mselect_context\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mapp\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    482\u001B[0m         \u001B[0;32melif\u001B[0m \u001B[0mtype\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mapp\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__module__\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstartswith\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"llama_index\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    483\u001B[0m             \u001B[0;32mfrom\u001B[0m \u001B[0mtrulens_eval\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtru_llama\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mTruLlama\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.10/dist-packages/trulens_eval/tru_chain.py\u001B[0m in \u001B[0;36mselect_context\u001B[0;34m(cls, app)\u001B[0m\n\u001B[1;32m    246\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    247\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mretrievers\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m>\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 248\u001B[0;31m             raise ValueError(\n\u001B[0m\u001B[1;32m    249\u001B[0m                 \u001B[0;34m\"Found more than one `BaseRetriever` in app:\\n\\t\"\u001B[0m \u001B[0;34m+\u001B[0m\u001B[0;31m \u001B[0m\u001B[0;31m\\\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    250\u001B[0m                 (\"\\n\\t\".join(map(\n",
      "\u001B[0;31mValueError\u001B[0m: Found more than one `BaseRetriever` in app:\n\t<class 'langchain.retrievers.contextual_compression.ContextualCompressionRetriever'> at first.steps.context.first\n\t<class 'langchain_core.vectorstores.VectorStoreRetriever'> at first.steps.context.first.base_retriever"
     ]
    }
   ],
   "source": [
    "\n",
    "context = App.select_context(rag_chain_compressed)\n",
    "\n",
    "from trulens_eval.feedback import Groundedness\n",
    "grounded = Groundedness(groundedness_provider=OpenAI())\n",
    "# Define a groundedness feedback function\n",
    "f_groundedness = (\n",
    "    Feedback(grounded.groundedness_measure_with_cot_reasons)\n",
    "    .on(context.collect()) # collect context chunks into a list\n",
    "    .on_output()\n",
    "    .aggregate(grounded.grounded_statements_aggregator)\n",
    ")\n",
    "\n",
    "# Question/answer relevance between overall question and answer.\n",
    "f_qa_relevance = Feedback(openai.relevance).on_input_output()\n",
    "# Question/statement relevance between question and each context chunk.\n",
    "f_context_relevance = (\n",
    "    Feedback(openai.qs_relevance)\n",
    "    .on_input()\n",
    "    .on(context)\n",
    "    .aggregate(np.mean)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jcspfzMwnb4C",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Instrument chain for logging with TruLens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "8mpBOa1Inb4C",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tru_recorder = TruChain(rag_chain,\n",
    "    app_id='Chain1_ChatApplication',\n",
    "    feedbacks=[f_qa_relevance, f_context_relevance, f_groundedness])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "rQMUdGzg1t9h",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tru_recorder2 = TruChain(rag_chain_compressed,\n",
    "    app_id='Chain2_ChatApplication',\n",
    "    feedbacks=[f_qa_relevance, f_context_relevance, f_groundedness])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 125
    },
    "id": "j0dzMHfdnb4C",
    "outputId": "8e47f1e6-4019-4e5b-d490-34fbb3ad17bd",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new object of type <class 'langchain_core.runnables.base.RunnableSequence'> at 0x16a7c97c0 is calling an instrumented method <function RunnableSequence.invoke at 0x154ce8af0>. The path of this call may be incorrect.\n",
      "Guessing path of new object is app based on other object (0x16a3105c0) using this function.\n",
      "A new object of type <class 'langchain_core.runnables.base.RunnableParallel'> at 0x16a927540 is calling an instrumented method <function RunnableParallel.invoke at 0x154ce9870>. The path of this call may be incorrect.\n",
      "Guessing path of new object is app.first based on other object (0x16c193c00) using this function.\n",
      "A new object of type <class 'langchain_core.runnables.base.RunnableSequence'> at 0x169b94a80 is calling an instrumented method <function RunnableSequence.invoke at 0x154ce8af0>. The path of this call may be incorrect.\n",
      "Guessing path of new object is app based on other object (0x16a3105c0) using this function.\n",
      "A new object of type <class 'langchain_core.runnables.passthrough.RunnablePassthrough'> at 0x169f1e180 is calling an instrumented method <function RunnablePassthrough.invoke at 0x154d164d0>. The path of this call may be incorrect.\n",
      "Guessing path of new object is app.first.steps.question based on other object (0x169ee3140) using this function.\n",
      "A new object of type <class 'langchain_core.vectorstores.VectorStoreRetriever'> at 0x1699a42c0 is calling an instrumented method <function BaseRetriever.invoke at 0x15633be20>. The path of this call may be incorrect.\n",
      "Guessing path of new object is app.first.steps.context.first based on other object (0x1699b5440) using this function.\n",
      "A new object of type <class 'langchain_core.vectorstores.VectorStoreRetriever'> at 0x1699a42c0 is calling an instrumented method <function BaseRetriever.get_relevant_documents at 0x1563700d0>. The path of this call may be incorrect.\n",
      "Guessing path of new object is app.first.steps.context.first based on other object (0x1699b5440) using this function.\n",
      "A new object of type <class 'langchain_core.vectorstores.VectorStoreRetriever'> at 0x1699a42c0 is calling an instrumented method <function VectorStoreRetriever._get_relevant_documents at 0x1573ea7a0>. The path of this call may be incorrect.\n",
      "Guessing path of new object is app.first.steps.context.first.base_retriever based on other object (0x16a3117c0) using this function.\n",
      "A new object of type <class 'langchain_core.output_parsers.string.StrOutputParser'> at 0x16a8d2a80 is calling an instrumented method <function BaseOutputParser.invoke at 0x154d48310>. The path of this call may be incorrect.\n",
      "Guessing path of new object is app.last based on other object (0x16a985a00) using this function.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The purpose of the source is not clear from the given context.'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with tru_recorder as recording:\n",
    "    llm_response = rag_chain.invoke(\"What is the purpose of the source?\")\n",
    "\n",
    "display(llm_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new object of type <class 'langchain_core.vectorstores.VectorStoreRetriever'> at 0x1699a42c0 is calling an instrumented method <function BaseRetriever.get_relevant_documents at 0x1563700d0>. The path of this call may be incorrect.\n",
      "Guessing path of new object is app.first.steps.context.first based on other object (0x1699b5440) using this function.\n",
      "A new object of type <class 'langchain_core.vectorstores.VectorStoreRetriever'> at 0x1699a42c0 is calling an instrumented method <function VectorStoreRetriever._get_relevant_documents at 0x1573ea7a0>. The path of this call may be incorrect.\n",
      "Guessing path of new object is app.first.steps.context.first.base_retriever based on other object (0x16a3117c0) using this function.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The purpose of the source is not clear from the given context.'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with tru_recorder2 as recording:\n",
    "    llm_response = rag_chain.invoke(\"What is the purpose of the source?\")\n",
    "\n",
    "display(llm_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nMHp9RDonb4C",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Retrieve records and feedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "obcL4p_Nnb4C",
    "outputId": "8cf8fb4e-5263-42ae-ffc0-d15767d860ca",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Record(record_id='record_hash_dfd61e84c359e9d694aa6d903e52967e', app_id='Chain2_ChatApplication', cost=Cost(n_requests=2, n_successful_requests=2, n_classes=0, n_tokens=502, n_stream_chunks=0, n_prompt_tokens=489, n_completion_tokens=13, cost=0.0007475), perf=Perf(start_time=datetime.datetime(2024, 2, 3, 10, 50, 26, 190994), end_time=datetime.datetime(2024, 2, 3, 10, 50, 28, 869297)), ts=datetime.datetime(2024, 2, 3, 10, 50, 28, 869379), tags='-', meta=None, main_input='What is the purpose of the source?', main_output='The purpose of the source is not clear from the given context.', main_error=None, calls=[RecordAppCall(stack=[RecordAppCallMethod(path=Lens().app, method=Method(obj=Obj(cls=langchain_core.runnables.base.RunnableSequence, id=6081517504, init_bindings=None), name='invoke')), RecordAppCallMethod(path=Lens().app.first, method=Method(obj=Obj(cls=langchain_core.runnables.base.RunnableParallel, id=6082950464, init_bindings=None), name='invoke')), RecordAppCallMethod(path=Lens().app.first.steps.question, method=Method(obj=Obj(cls=langchain_core.runnables.passthrough.RunnablePassthrough, id=6072426880, init_bindings=None), name='invoke'))], args={'input': 'What is the purpose of the source?', 'config': {'tags': [], 'metadata': {}, 'callbacks': {'__tru_non_serialized_object': {'cls': {'name': 'CallbackManager', 'module': {'package_name': 'langchain_core.callbacks', 'module_name': 'langchain_core.callbacks.manager'}, 'bases': None}, 'id': 6369353424, 'init_bindings': None}}, 'recursion_limit': 25, 'configurable': {}}}, rets='What is the purpose of the source?', error=None, perf=Perf(start_time=datetime.datetime(2024, 2, 3, 10, 50, 26, 516461), end_time=datetime.datetime(2024, 2, 3, 10, 50, 26, 597082)), pid=47537, tid=1391955), RecordAppCall(stack=[RecordAppCallMethod(path=Lens().app, method=Method(obj=Obj(cls=langchain_core.runnables.base.RunnableSequence, id=6081517504, init_bindings=None), name='invoke')), RecordAppCallMethod(path=Lens().app.first, method=Method(obj=Obj(cls=langchain_core.runnables.base.RunnableParallel, id=6082950464, init_bindings=None), name='invoke')), RecordAppCallMethod(path=Lens().app, method=Method(obj=Obj(cls=langchain_core.runnables.base.RunnableSequence, id=6081517504, init_bindings=None), name='invoke')), RecordAppCallMethod(path=Lens().app.first.steps.context.first, method=Method(obj=Obj(cls=langchain_core.vectorstores.VectorStoreRetriever, id=6066684608, init_bindings=None), name='invoke')), RecordAppCallMethod(path=Lens().app.first.steps.context.first, method=Method(obj=Obj(cls=langchain_core.vectorstores.VectorStoreRetriever, id=6066684608, init_bindings=None), name='get_relevant_documents')), RecordAppCallMethod(path=Lens().app.first.steps.context.first.base_retriever, method=Method(obj=Obj(cls=langchain_core.vectorstores.VectorStoreRetriever, id=6066684608, init_bindings=None), name='_get_relevant_documents'))], args={'query': 'What is the purpose of the source?', 'run_manager': {'__tru_non_serialized_object': {'cls': {'name': 'CallbackManagerForRetrieverRun', 'module': {'package_name': 'langchain_core.callbacks', 'module_name': 'langchain_core.callbacks.manager'}, 'bases': None}, 'id': 6370022096, 'init_bindings': None}}}, rets=[{'page_content': '}\\n]\\nThen after these clarification, the agent moved into the code writing mode with a different system message.\\nSystem message:', 'metadata': {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, 'type': 'Document'}, {'page_content': 'Resources:\\n1. Internet access for searches and information gathering.\\n2. Long Term memory management.\\n3. GPT-3.5 powered Agents for delegation of simple tasks.\\n4. File output.\\n\\nPerformance Evaluation:\\n1. Continuously review and analyze your actions to ensure you are performing to the best of your abilities.\\n2. Constructively self-criticize your big-picture behavior constantly.\\n3. Reflect on past decisions and strategies to refine your approach.\\n4. Every command has a cost, so be smart and efficient. Aim to complete tasks in the least number of steps.', 'metadata': {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, 'type': 'Document'}, {'page_content': 'Fig. 7. Comparison of AD, ED, source policy and RL^2 on environments that require memory and exploration. Only binary reward is assigned. The source policies are trained with A3C for \"dark\" environments and DQN for watermaze.(Image source: Laskin et al. 2023)\\nComponent Two: Memory#\\n(Big thank you to ChatGPT for helping me draft this section. I’ve learned a lot about the human brain and data structure for fast MIPS in my conversations with ChatGPT.)\\nTypes of Memory#\\nMemory can be defined as the processes used to acquire, store, retain, and later retrieve information. There are several types of memory in human brains.\\n\\n\\nSensory Memory: This is the earliest stage of memory, providing the ability to retain impressions of sensory information (visual, auditory, etc) after the original stimuli have ended. Sensory memory typically only lasts for up to a few seconds. Subcategories include iconic memory (visual), echoic memory (auditory), and haptic memory (touch).', 'metadata': {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, 'type': 'Document'}, {'page_content': 'You always add a comment briefly describing the purpose of the function definition.\\nYou try to add comments explaining very complex bits of logic.\\nYou always follow the best practices for the requested languages in terms of describing the code written as a defined\\npackage/project.\\nPython toolbelt preferences:', 'metadata': {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, 'type': 'Document'}], error=None, perf=Perf(start_time=datetime.datetime(2024, 2, 3, 10, 50, 26, 712092), end_time=datetime.datetime(2024, 2, 3, 10, 50, 27, 80575)), pid=47537, tid=1391952), RecordAppCall(stack=[RecordAppCallMethod(path=Lens().app, method=Method(obj=Obj(cls=langchain_core.runnables.base.RunnableSequence, id=6081517504, init_bindings=None), name='invoke')), RecordAppCallMethod(path=Lens().app.first, method=Method(obj=Obj(cls=langchain_core.runnables.base.RunnableParallel, id=6082950464, init_bindings=None), name='invoke')), RecordAppCallMethod(path=Lens().app, method=Method(obj=Obj(cls=langchain_core.runnables.base.RunnableSequence, id=6081517504, init_bindings=None), name='invoke')), RecordAppCallMethod(path=Lens().app.first.steps.context.first, method=Method(obj=Obj(cls=langchain_core.vectorstores.VectorStoreRetriever, id=6066684608, init_bindings=None), name='invoke')), RecordAppCallMethod(path=Lens().app.first.steps.context.first, method=Method(obj=Obj(cls=langchain_core.vectorstores.VectorStoreRetriever, id=6066684608, init_bindings=None), name='get_relevant_documents'))], args={'query': 'What is the purpose of the source?', 'callbacks': {'__tru_non_serialized_object': {'cls': {'name': 'CallbackManager', 'module': {'package_name': 'langchain_core.callbacks', 'module_name': 'langchain_core.callbacks.manager'}, 'bases': None}, 'id': 6369918496, 'init_bindings': None}}, 'tags': [], 'metadata': {}, 'run_name': None}, rets=[{'page_content': '}\\n]\\nThen after these clarification, the agent moved into the code writing mode with a different system message.\\nSystem message:', 'metadata': {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, 'type': 'Document'}, {'page_content': 'Resources:\\n1. Internet access for searches and information gathering.\\n2. Long Term memory management.\\n3. GPT-3.5 powered Agents for delegation of simple tasks.\\n4. File output.\\n\\nPerformance Evaluation:\\n1. Continuously review and analyze your actions to ensure you are performing to the best of your abilities.\\n2. Constructively self-criticize your big-picture behavior constantly.\\n3. Reflect on past decisions and strategies to refine your approach.\\n4. Every command has a cost, so be smart and efficient. Aim to complete tasks in the least number of steps.', 'metadata': {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, 'type': 'Document'}, {'page_content': 'Fig. 7. Comparison of AD, ED, source policy and RL^2 on environments that require memory and exploration. Only binary reward is assigned. The source policies are trained with A3C for \"dark\" environments and DQN for watermaze.(Image source: Laskin et al. 2023)\\nComponent Two: Memory#\\n(Big thank you to ChatGPT for helping me draft this section. I’ve learned a lot about the human brain and data structure for fast MIPS in my conversations with ChatGPT.)\\nTypes of Memory#\\nMemory can be defined as the processes used to acquire, store, retain, and later retrieve information. There are several types of memory in human brains.\\n\\n\\nSensory Memory: This is the earliest stage of memory, providing the ability to retain impressions of sensory information (visual, auditory, etc) after the original stimuli have ended. Sensory memory typically only lasts for up to a few seconds. Subcategories include iconic memory (visual), echoic memory (auditory), and haptic memory (touch).', 'metadata': {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, 'type': 'Document'}, {'page_content': 'You always add a comment briefly describing the purpose of the function definition.\\nYou try to add comments explaining very complex bits of logic.\\nYou always follow the best practices for the requested languages in terms of describing the code written as a defined\\npackage/project.\\nPython toolbelt preferences:', 'metadata': {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, 'type': 'Document'}], error=None, perf=Perf(start_time=datetime.datetime(2024, 2, 3, 10, 50, 26, 634762), end_time=datetime.datetime(2024, 2, 3, 10, 50, 27, 81971)), pid=47537, tid=1391952), RecordAppCall(stack=[RecordAppCallMethod(path=Lens().app, method=Method(obj=Obj(cls=langchain_core.runnables.base.RunnableSequence, id=6081517504, init_bindings=None), name='invoke')), RecordAppCallMethod(path=Lens().app.first, method=Method(obj=Obj(cls=langchain_core.runnables.base.RunnableParallel, id=6082950464, init_bindings=None), name='invoke')), RecordAppCallMethod(path=Lens().app, method=Method(obj=Obj(cls=langchain_core.runnables.base.RunnableSequence, id=6081517504, init_bindings=None), name='invoke')), RecordAppCallMethod(path=Lens().app.first.steps.context.first, method=Method(obj=Obj(cls=langchain_core.vectorstores.VectorStoreRetriever, id=6066684608, init_bindings=None), name='invoke'))], args={'input': 'What is the purpose of the source?', 'config': {'tags': [], 'metadata': {}, 'callbacks': {'__tru_non_serialized_object': {'cls': {'name': 'CallbackManager', 'module': {'package_name': 'langchain_core.callbacks', 'module_name': 'langchain_core.callbacks.manager'}, 'bases': None}, 'id': 6369918496, 'init_bindings': None}}, 'recursion_limit': 25, 'configurable': {}}}, rets=[{'page_content': '}\\n]\\nThen after these clarification, the agent moved into the code writing mode with a different system message.\\nSystem message:', 'metadata': {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, 'type': 'Document'}, {'page_content': 'Resources:\\n1. Internet access for searches and information gathering.\\n2. Long Term memory management.\\n3. GPT-3.5 powered Agents for delegation of simple tasks.\\n4. File output.\\n\\nPerformance Evaluation:\\n1. Continuously review and analyze your actions to ensure you are performing to the best of your abilities.\\n2. Constructively self-criticize your big-picture behavior constantly.\\n3. Reflect on past decisions and strategies to refine your approach.\\n4. Every command has a cost, so be smart and efficient. Aim to complete tasks in the least number of steps.', 'metadata': {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, 'type': 'Document'}, {'page_content': 'Fig. 7. Comparison of AD, ED, source policy and RL^2 on environments that require memory and exploration. Only binary reward is assigned. The source policies are trained with A3C for \"dark\" environments and DQN for watermaze.(Image source: Laskin et al. 2023)\\nComponent Two: Memory#\\n(Big thank you to ChatGPT for helping me draft this section. I’ve learned a lot about the human brain and data structure for fast MIPS in my conversations with ChatGPT.)\\nTypes of Memory#\\nMemory can be defined as the processes used to acquire, store, retain, and later retrieve information. There are several types of memory in human brains.\\n\\n\\nSensory Memory: This is the earliest stage of memory, providing the ability to retain impressions of sensory information (visual, auditory, etc) after the original stimuli have ended. Sensory memory typically only lasts for up to a few seconds. Subcategories include iconic memory (visual), echoic memory (auditory), and haptic memory (touch).', 'metadata': {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, 'type': 'Document'}, {'page_content': 'You always add a comment briefly describing the purpose of the function definition.\\nYou try to add comments explaining very complex bits of logic.\\nYou always follow the best practices for the requested languages in terms of describing the code written as a defined\\npackage/project.\\nPython toolbelt preferences:', 'metadata': {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, 'type': 'Document'}], error=None, perf=Perf(start_time=datetime.datetime(2024, 2, 3, 10, 50, 26, 567816), end_time=datetime.datetime(2024, 2, 3, 10, 50, 27, 82871)), pid=47537, tid=1391952), RecordAppCall(stack=[RecordAppCallMethod(path=Lens().app, method=Method(obj=Obj(cls=langchain_core.runnables.base.RunnableSequence, id=6081517504, init_bindings=None), name='invoke')), RecordAppCallMethod(path=Lens().app.first, method=Method(obj=Obj(cls=langchain_core.runnables.base.RunnableParallel, id=6082950464, init_bindings=None), name='invoke')), RecordAppCallMethod(path=Lens().app, method=Method(obj=Obj(cls=langchain_core.runnables.base.RunnableSequence, id=6081517504, init_bindings=None), name='invoke'))], args={'input': 'What is the purpose of the source?', 'config': {'tags': [], 'metadata': {}, 'callbacks': {'__tru_non_serialized_object': {'cls': {'name': 'CallbackManager', 'module': {'package_name': 'langchain_core.callbacks', 'module_name': 'langchain_core.callbacks.manager'}, 'bases': None}, 'id': 6369354672, 'init_bindings': None}}, 'recursion_limit': 25, 'configurable': {}}}, rets='}\\n]\\nThen after these clarification, the agent moved into the code writing mode with a different system message.\\nSystem message:\\n\\nResources:\\n1. Internet access for searches and information gathering.\\n2. Long Term memory management.\\n3. GPT-3.5 powered Agents for delegation of simple tasks.\\n4. File output.\\n\\nPerformance Evaluation:\\n1. Continuously review and analyze your actions to ensure you are performing to the best of your abilities.\\n2. Constructively self-criticize your big-picture behavior constantly.\\n3. Reflect on past decisions and strategies to refine your approach.\\n4. Every command has a cost, so be smart and efficient. Aim to complete tasks in the least number of steps.\\n\\nFig. 7. Comparison of AD, ED, source policy and RL^2 on environments that require memory and exploration. Only binary reward is assigned. The source policies are trained with A3C for \"dark\" environments and DQN for watermaze.(Image source: Laskin et al. 2023)\\nComponent Two: Memory#\\n(Big thank you to ChatGPT for helping me draft this section. I’ve learned a lot about the human brain and data structure for fast MIPS in my conversations with ChatGPT.)\\nTypes of Memory#\\nMemory can be defined as the processes used to acquire, store, retain, and later retrieve information. There are several types of memory in human brains.\\n\\n\\nSensory Memory: This is the earliest stage of memory, providing the ability to retain impressions of sensory information (visual, auditory, etc) after the original stimuli have ended. Sensory memory typically only lasts for up to a few seconds. Subcategories include iconic memory (visual), echoic memory (auditory), and haptic memory (touch).\\n\\nYou always add a comment briefly describing the purpose of the function definition.\\nYou try to add comments explaining very complex bits of logic.\\nYou always follow the best practices for the requested languages in terms of describing the code written as a defined\\npackage/project.\\nPython toolbelt preferences:', error=None, perf=Perf(start_time=datetime.datetime(2024, 2, 3, 10, 50, 26, 473759), end_time=datetime.datetime(2024, 2, 3, 10, 50, 27, 84873)), pid=47537, tid=1391952), RecordAppCall(stack=[RecordAppCallMethod(path=Lens().app, method=Method(obj=Obj(cls=langchain_core.runnables.base.RunnableSequence, id=6081517504, init_bindings=None), name='invoke')), RecordAppCallMethod(path=Lens().app.first, method=Method(obj=Obj(cls=langchain_core.runnables.base.RunnableParallel, id=6082950464, init_bindings=None), name='invoke'))], args={'input': 'What is the purpose of the source?', 'config': {'tags': [], 'metadata': {}, 'callbacks': {'__tru_non_serialized_object': {'cls': {'name': 'CallbackManager', 'module': {'package_name': 'langchain_core.callbacks', 'module_name': 'langchain_core.callbacks.manager'}, 'bases': None}, 'id': 6369378944, 'init_bindings': None}}, 'recursion_limit': 25, 'configurable': {}}}, rets={'context': '}\\n]\\nThen after these clarification, the agent moved into the code writing mode with a different system message.\\nSystem message:\\n\\nResources:\\n1. Internet access for searches and information gathering.\\n2. Long Term memory management.\\n3. GPT-3.5 powered Agents for delegation of simple tasks.\\n4. File output.\\n\\nPerformance Evaluation:\\n1. Continuously review and analyze your actions to ensure you are performing to the best of your abilities.\\n2. Constructively self-criticize your big-picture behavior constantly.\\n3. Reflect on past decisions and strategies to refine your approach.\\n4. Every command has a cost, so be smart and efficient. Aim to complete tasks in the least number of steps.\\n\\nFig. 7. Comparison of AD, ED, source policy and RL^2 on environments that require memory and exploration. Only binary reward is assigned. The source policies are trained with A3C for \"dark\" environments and DQN for watermaze.(Image source: Laskin et al. 2023)\\nComponent Two: Memory#\\n(Big thank you to ChatGPT for helping me draft this section. I’ve learned a lot about the human brain and data structure for fast MIPS in my conversations with ChatGPT.)\\nTypes of Memory#\\nMemory can be defined as the processes used to acquire, store, retain, and later retrieve information. There are several types of memory in human brains.\\n\\n\\nSensory Memory: This is the earliest stage of memory, providing the ability to retain impressions of sensory information (visual, auditory, etc) after the original stimuli have ended. Sensory memory typically only lasts for up to a few seconds. Subcategories include iconic memory (visual), echoic memory (auditory), and haptic memory (touch).\\n\\nYou always add a comment briefly describing the purpose of the function definition.\\nYou try to add comments explaining very complex bits of logic.\\nYou always follow the best practices for the requested languages in terms of describing the code written as a defined\\npackage/project.\\nPython toolbelt preferences:', 'question': 'What is the purpose of the source?'}, error=None, perf=Perf(start_time=datetime.datetime(2024, 2, 3, 10, 50, 26, 304419), end_time=datetime.datetime(2024, 2, 3, 10, 50, 27, 85828)), pid=47537, tid=1387381), RecordAppCall(stack=[RecordAppCallMethod(path=Lens().app, method=Method(obj=Obj(cls=langchain_core.runnables.base.RunnableSequence, id=6081517504, init_bindings=None), name='invoke')), RecordAppCallMethod(path=Lens().app.middle[0], method=Method(obj=Obj(cls=langchain_core.prompts.chat.ChatPromptTemplate, id=6072168384, init_bindings=None), name='invoke'))], args={'input': {'context': '}\\n]\\nThen after these clarification, the agent moved into the code writing mode with a different system message.\\nSystem message:\\n\\nResources:\\n1. Internet access for searches and information gathering.\\n2. Long Term memory management.\\n3. GPT-3.5 powered Agents for delegation of simple tasks.\\n4. File output.\\n\\nPerformance Evaluation:\\n1. Continuously review and analyze your actions to ensure you are performing to the best of your abilities.\\n2. Constructively self-criticize your big-picture behavior constantly.\\n3. Reflect on past decisions and strategies to refine your approach.\\n4. Every command has a cost, so be smart and efficient. Aim to complete tasks in the least number of steps.\\n\\nFig. 7. Comparison of AD, ED, source policy and RL^2 on environments that require memory and exploration. Only binary reward is assigned. The source policies are trained with A3C for \"dark\" environments and DQN for watermaze.(Image source: Laskin et al. 2023)\\nComponent Two: Memory#\\n(Big thank you to ChatGPT for helping me draft this section. I’ve learned a lot about the human brain and data structure for fast MIPS in my conversations with ChatGPT.)\\nTypes of Memory#\\nMemory can be defined as the processes used to acquire, store, retain, and later retrieve information. There are several types of memory in human brains.\\n\\n\\nSensory Memory: This is the earliest stage of memory, providing the ability to retain impressions of sensory information (visual, auditory, etc) after the original stimuli have ended. Sensory memory typically only lasts for up to a few seconds. Subcategories include iconic memory (visual), echoic memory (auditory), and haptic memory (touch).\\n\\nYou always add a comment briefly describing the purpose of the function definition.\\nYou try to add comments explaining very complex bits of logic.\\nYou always follow the best practices for the requested languages in terms of describing the code written as a defined\\npackage/project.\\nPython toolbelt preferences:', 'question': 'What is the purpose of the source?'}, 'config': {'tags': [], 'metadata': {}, 'callbacks': {'__tru_non_serialized_object': {'cls': {'name': 'CallbackManager', 'module': {'package_name': 'langchain_core.callbacks', 'module_name': 'langchain_core.callbacks.manager'}, 'bases': None}, 'id': 6375315376, 'init_bindings': None}}, 'recursion_limit': 25, 'configurable': {}}}, rets={'messages': [{'content': 'You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don\\'t know the answer, just say that you don\\'t know. Use three sentences maximum and keep the answer concise.\\nQuestion: What is the purpose of the source? \\nContext: }\\n]\\nThen after these clarification, the agent moved into the code writing mode with a different system message.\\nSystem message:\\n\\nResources:\\n1. Internet access for searches and information gathering.\\n2. Long Term memory management.\\n3. GPT-3.5 powered Agents for delegation of simple tasks.\\n4. File output.\\n\\nPerformance Evaluation:\\n1. Continuously review and analyze your actions to ensure you are performing to the best of your abilities.\\n2. Constructively self-criticize your big-picture behavior constantly.\\n3. Reflect on past decisions and strategies to refine your approach.\\n4. Every command has a cost, so be smart and efficient. Aim to complete tasks in the least number of steps.\\n\\nFig. 7. Comparison of AD, ED, source policy and RL^2 on environments that require memory and exploration. Only binary reward is assigned. The source policies are trained with A3C for \"dark\" environments and DQN for watermaze.(Image source: Laskin et al. 2023)\\nComponent Two: Memory#\\n(Big thank you to ChatGPT for helping me draft this section. I’ve learned a lot about the human brain and data structure for fast MIPS in my conversations with ChatGPT.)\\nTypes of Memory#\\nMemory can be defined as the processes used to acquire, store, retain, and later retrieve information. There are several types of memory in human brains.\\n\\n\\nSensory Memory: This is the earliest stage of memory, providing the ability to retain impressions of sensory information (visual, auditory, etc) after the original stimuli have ended. Sensory memory typically only lasts for up to a few seconds. Subcategories include iconic memory (visual), echoic memory (auditory), and haptic memory (touch).\\n\\nYou always add a comment briefly describing the purpose of the function definition.\\nYou try to add comments explaining very complex bits of logic.\\nYou always follow the best practices for the requested languages in terms of describing the code written as a defined\\npackage/project.\\nPython toolbelt preferences: \\nAnswer:', 'additional_kwargs': {}, 'type': 'human', 'example': False}]}, error=None, perf=Perf(start_time=datetime.datetime(2024, 2, 3, 10, 50, 27, 201274), end_time=datetime.datetime(2024, 2, 3, 10, 50, 27, 322823)), pid=47537, tid=1387381), RecordAppCall(stack=[RecordAppCallMethod(path=Lens().app, method=Method(obj=Obj(cls=langchain_core.runnables.base.RunnableSequence, id=6081517504, init_bindings=None), name='invoke')), RecordAppCallMethod(path=Lens().app.middle[1], method=Method(obj=Obj(cls=langchain_community.chat_models.openai.ChatOpenAI, id=4393521664, init_bindings=None), name='invoke'))], args={'input': {'messages': [{'content': 'You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don\\'t know the answer, just say that you don\\'t know. Use three sentences maximum and keep the answer concise.\\nQuestion: What is the purpose of the source? \\nContext: }\\n]\\nThen after these clarification, the agent moved into the code writing mode with a different system message.\\nSystem message:\\n\\nResources:\\n1. Internet access for searches and information gathering.\\n2. Long Term memory management.\\n3. GPT-3.5 powered Agents for delegation of simple tasks.\\n4. File output.\\n\\nPerformance Evaluation:\\n1. Continuously review and analyze your actions to ensure you are performing to the best of your abilities.\\n2. Constructively self-criticize your big-picture behavior constantly.\\n3. Reflect on past decisions and strategies to refine your approach.\\n4. Every command has a cost, so be smart and efficient. Aim to complete tasks in the least number of steps.\\n\\nFig. 7. Comparison of AD, ED, source policy and RL^2 on environments that require memory and exploration. Only binary reward is assigned. The source policies are trained with A3C for \"dark\" environments and DQN for watermaze.(Image source: Laskin et al. 2023)\\nComponent Two: Memory#\\n(Big thank you to ChatGPT for helping me draft this section. I’ve learned a lot about the human brain and data structure for fast MIPS in my conversations with ChatGPT.)\\nTypes of Memory#\\nMemory can be defined as the processes used to acquire, store, retain, and later retrieve information. There are several types of memory in human brains.\\n\\n\\nSensory Memory: This is the earliest stage of memory, providing the ability to retain impressions of sensory information (visual, auditory, etc) after the original stimuli have ended. Sensory memory typically only lasts for up to a few seconds. Subcategories include iconic memory (visual), echoic memory (auditory), and haptic memory (touch).\\n\\nYou always add a comment briefly describing the purpose of the function definition.\\nYou try to add comments explaining very complex bits of logic.\\nYou always follow the best practices for the requested languages in terms of describing the code written as a defined\\npackage/project.\\nPython toolbelt preferences: \\nAnswer:', 'additional_kwargs': {}, 'type': 'human', 'example': False}]}, 'config': {'tags': [], 'metadata': {}, 'callbacks': {'__tru_non_serialized_object': {'cls': {'name': 'CallbackManager', 'module': {'package_name': 'langchain_core.callbacks', 'module_name': 'langchain_core.callbacks.manager'}, 'bases': None}, 'id': 6379148512, 'init_bindings': None}}, 'recursion_limit': 25, 'configurable': {}}}, rets={'content': 'The purpose of the source is not clear from the given context.', 'additional_kwargs': {}, 'type': 'ai', 'example': False}, error=None, perf=Perf(start_time=datetime.datetime(2024, 2, 3, 10, 50, 27, 349695), end_time=datetime.datetime(2024, 2, 3, 10, 50, 28, 806833)), pid=47537, tid=1387381), RecordAppCall(stack=[RecordAppCallMethod(path=Lens().app, method=Method(obj=Obj(cls=langchain_core.runnables.base.RunnableSequence, id=6081517504, init_bindings=None), name='invoke')), RecordAppCallMethod(path=Lens().app.last, method=Method(obj=Obj(cls=langchain_core.output_parsers.string.StrOutputParser, id=6082603648, init_bindings=None), name='invoke'))], args={'input': {'content': 'The purpose of the source is not clear from the given context.', 'additional_kwargs': {}, 'type': 'ai', 'example': False}, 'config': {'tags': [], 'metadata': {}, 'callbacks': {'__tru_non_serialized_object': {'cls': {'name': 'CallbackManager', 'module': {'package_name': 'langchain_core.callbacks', 'module_name': 'langchain_core.callbacks.manager'}, 'bases': None}, 'id': 6379148512, 'init_bindings': None}}, 'recursion_limit': 25, 'configurable': {}}}, rets='The purpose of the source is not clear from the given context.', error=None, perf=Perf(start_time=datetime.datetime(2024, 2, 3, 10, 50, 28, 830602), end_time=datetime.datetime(2024, 2, 3, 10, 50, 28, 868707)), pid=47537, tid=1387381), RecordAppCall(stack=[RecordAppCallMethod(path=Lens().app, method=Method(obj=Obj(cls=langchain_core.runnables.base.RunnableSequence, id=6081517504, init_bindings=None), name='invoke'))], args={'input': 'What is the purpose of the source?'}, rets='The purpose of the source is not clear from the given context.', error=None, perf=Perf(start_time=datetime.datetime(2024, 2, 3, 10, 50, 26, 190994), end_time=datetime.datetime(2024, 2, 3, 10, 50, 28, 869297)), pid=47537, tid=1387381)], feedback_results=[<Future at 0x17ba2e230 state=finished returned tuple>, <Future at 0x17ba2f640 state=finished returned tuple>, <Future at 0x17ba2fee0 state=finished returned tuple>])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# The record of the app invocation can be retrieved from the `recording`:\n",
    "\n",
    "rec = recording.get() # use .get if only one record\n",
    "# recs = recording.records # use .records if multiple\n",
    "\n",
    "display(rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 124
    },
    "id": "ORHXtj6znb4D",
    "outputId": "e523702b-0e5d-4092-f9ba-31a74c0061c1",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'groundedness_measure_with_cot_reasons'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'qs_relevance'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.2"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'relevance'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# The results of the feedback functions can be rertireved from the record. These\n",
    "# are `Future` instances (see `concurrent.futures`). You can use `as_completed`\n",
    "# to wait until they have finished evaluating.\n",
    "\n",
    "from concurrent.futures import as_completed\n",
    "\n",
    "for feedback_future in  as_completed(rec.feedback_results):\n",
    "    feedback, feedback_result = feedback_future.result()\n",
    "\n",
    "    feedback: Feedback\n",
    "    feedbac_result: FeedbackResult\n",
    "\n",
    "    display(feedback.name, feedback_result.result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 153
    },
    "id": "_1f3agnrnb4D",
    "outputId": "7c3f37f0-eb75-4c42-8f53-843f59494c9e",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>app_id</th>\n",
       "      <th>app_json</th>\n",
       "      <th>type</th>\n",
       "      <th>record_id</th>\n",
       "      <th>input</th>\n",
       "      <th>output</th>\n",
       "      <th>tags</th>\n",
       "      <th>record_json</th>\n",
       "      <th>cost_json</th>\n",
       "      <th>perf_json</th>\n",
       "      <th>ts</th>\n",
       "      <th>relevance</th>\n",
       "      <th>qs_relevance</th>\n",
       "      <th>groundedness_measure_with_cot_reasons</th>\n",
       "      <th>relevance_calls</th>\n",
       "      <th>qs_relevance_calls</th>\n",
       "      <th>groundedness_measure_with_cot_reasons_calls</th>\n",
       "      <th>latency</th>\n",
       "      <th>total_tokens</th>\n",
       "      <th>total_cost</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Chain2_ChatApplication</td>\n",
       "      <td>{\"tru_class_info\": {\"name\": \"TruChain\", \"modul...</td>\n",
       "      <td>RunnableSequence(langchain_core.runnables.base)</td>\n",
       "      <td>record_hash_dfd61e84c359e9d694aa6d903e52967e</td>\n",
       "      <td>\"What is the purpose of the source?\"</td>\n",
       "      <td>\"The purpose of the source is not clear from t...</td>\n",
       "      <td>-</td>\n",
       "      <td>{\"record_id\": \"record_hash_dfd61e84c359e9d694a...</td>\n",
       "      <td>{\"n_requests\": 2, \"n_successful_requests\": 2, ...</td>\n",
       "      <td>{\"start_time\": \"2024-02-03T10:50:26.190994\", \"...</td>\n",
       "      <td>2024-02-03T10:50:28.869379</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[{'args': {'prompt': 'What is the purpose of t...</td>\n",
       "      <td>[{'args': {'question': 'What is the purpose of...</td>\n",
       "      <td>[{'args': {'source': [[{'page_content': '}\\n]\\...</td>\n",
       "      <td>2</td>\n",
       "      <td>502</td>\n",
       "      <td>0.000748</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   app_id                                           app_json  \\\n",
       "0  Chain2_ChatApplication  {\"tru_class_info\": {\"name\": \"TruChain\", \"modul...   \n",
       "\n",
       "                                              type  \\\n",
       "0  RunnableSequence(langchain_core.runnables.base)   \n",
       "\n",
       "                                      record_id  \\\n",
       "0  record_hash_dfd61e84c359e9d694aa6d903e52967e   \n",
       "\n",
       "                                  input  \\\n",
       "0  \"What is the purpose of the source?\"   \n",
       "\n",
       "                                              output tags  \\\n",
       "0  \"The purpose of the source is not clear from t...    -   \n",
       "\n",
       "                                         record_json  \\\n",
       "0  {\"record_id\": \"record_hash_dfd61e84c359e9d694a...   \n",
       "\n",
       "                                           cost_json  \\\n",
       "0  {\"n_requests\": 2, \"n_successful_requests\": 2, ...   \n",
       "\n",
       "                                           perf_json  \\\n",
       "0  {\"start_time\": \"2024-02-03T10:50:26.190994\", \"...   \n",
       "\n",
       "                           ts  relevance  qs_relevance  \\\n",
       "0  2024-02-03T10:50:28.869379        1.0           0.2   \n",
       "\n",
       "   groundedness_measure_with_cot_reasons  \\\n",
       "0                                    0.0   \n",
       "\n",
       "                                     relevance_calls  \\\n",
       "0  [{'args': {'prompt': 'What is the purpose of t...   \n",
       "\n",
       "                                  qs_relevance_calls  \\\n",
       "0  [{'args': {'question': 'What is the purpose of...   \n",
       "\n",
       "         groundedness_measure_with_cot_reasons_calls  latency  total_tokens  \\\n",
       "0  [{'args': {'source': [[{'page_content': '}\\n]\\...        2           502   \n",
       "\n",
       "   total_cost  \n",
       "0    0.000748  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "records, feedback = tru.get_records_and_feedback(app_ids=[\"Chain2_ChatApplication\"])\n",
    "\n",
    "records.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 112
    },
    "id": "aRLMc1nSnb4D",
    "outputId": "93fde643-aff6-4b13-eb98-8a12630936b9",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>relevance</th>\n",
       "      <th>groundedness_measure_with_cot_reasons</th>\n",
       "      <th>qs_relevance</th>\n",
       "      <th>latency</th>\n",
       "      <th>total_cost</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>app_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Chain2_ChatApplication</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.000748</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        relevance  groundedness_measure_with_cot_reasons  \\\n",
       "app_id                                                                     \n",
       "Chain2_ChatApplication        1.0                                    0.0   \n",
       "\n",
       "                        qs_relevance  latency  total_cost  \n",
       "app_id                                                     \n",
       "Chain2_ChatApplication           0.2      2.0    0.000748  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tru.get_leaderboard(app_ids=[\"Chain2_ChatApplication\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IPHUK9Xhnb4D",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Explore in a Dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mogbAPPlnb4D",
    "outputId": "20f859fd-0d48-4bc6-ef19-20b727050232",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting dashboard ...\n",
      "Config file already exists. Skipping writing process.\n",
      "Credentials file already exists. Skipping writing process.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94b59591902f4f08a36647267f287bea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Accordion(children=(VBox(children=(VBox(children=(Label(value='STDOUT'), Output())), VBox(children=(Label(valu…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dashboard started at http://10.10.130.79:8501 .\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Popen: returncode: None args: ['streamlit', 'run', '--server.headless=True'...>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tru.run_dashboard() # open a local streamlit app to explore\n",
    "\n",
    "# tru.stop_dashboard() # stop if needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5rN5O5hEnb4D",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Alternatively, you can run `trulens-eval` from a command line in the same folder to start the dashboard."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "olH4yXNYnb4D",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Note: Feedback functions evaluated in the deferred manner can be seen in the \"Progress\" page of the TruLens dashboard.\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "aienv",
   "language": "python",
   "name": "aienv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "d5737f6101ac92451320b0e41890107145710b89f85909f3780d702e7818f973"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}